{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Imports and setup\n",
    "\n",
    "The following is just boilerplate code that sets up the Spark session and sets some other non-essential configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf=SparkConf()\n",
    "\n",
    "# Load in a jar that provides extended string comparison functions such as Jaro Winkler.\n",
    "# Splink\n",
    "#     conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#     conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11')\n",
    "\n",
    "#sc = SparkContext.getOrCreate(conf=conf)\n",
    "#sc.setCheckpointDir(\"temp_graphframes/\")\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Entity Resolution with Lineage\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-1\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ['STORAGE'])\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .config(\"spark.jars\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register UDFs\n",
    "from pyspark.sql import types\n",
    "spark.udf.registerJavaFunction('jaro_winkler_sim', 'uk.gov.moj.dash.linkage.JaroWinklerSimilarity', types.DoubleType())\n",
    "spark.udf.registerJavaFunction('Dmetaphone', 'uk.gov.moj.dash.linkage.DoubleMetaphone', types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://100.100.184.150:20049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5.7.2.6.0-71</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://172.20.0.1:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Entity Resolution with Lineage</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe17c1c1cf8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig()  # Means logs will print in Jupyter Lab\n",
    "\n",
    "# Set to DEBUG if you want splink to log the SQL statements it's executing under the hood\n",
    "logging.getLogger(\"splink\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read in the data\n",
    "\n",
    "The `l` and `r` stand for 'left' and 'right.  It doesn't matter which of the two datasets you choose as the left, performance and results will be the same.\n",
    "\n",
    "‚ö†Ô∏è Note that `splink` makes the following assumptions about your data:\n",
    "\n",
    "-  There is a field containing a unique record identifier in each dataset\n",
    "-  The two datasets being linked have common column names - e.g. date of birth is represented in both datasets in a field of the same name.   In many cases, this means that the user needs to rename columns prior to using `splink`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### READING FROM PHOENIX INTO A SPARK DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Db:\n",
    "    def __init__(self):\n",
    "        opts = {}\n",
    "        opts['authentication'] = 'BASIC'\n",
    "        opts['avatica_user'] = os.environ[\"WORKLOAD_USER\"]\n",
    "        opts['avatica_password'] = os.environ[\"WORKLOAD_PASSWORD\"]\n",
    "        database_url = os.environ[\"OPDB_ENDPOINT_AWS2\"]\n",
    "        self.TABLENAME = \"test_table_paul\"\n",
    "        self.conn = phoenixdb.connect(database_url, autocommit=True,**opts)\n",
    "        self.curs = self.conn.cursor()\n",
    "        \n",
    "    def get_data(self):\n",
    "\n",
    "        query = f\"SELECT * FROM CML_WORKSHOP_TABLE_RIGHT\"\n",
    "\n",
    "        model.curs.execute(query)\n",
    "        rows = model.curs.fetchall()\n",
    "\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig( level=logging.DEBUG)\n",
    "\n",
    "import os\n",
    "import phoenixdb\n",
    "model = Db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_df = model.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('unique_id', StringType(), True),\n",
    "                     StructField('first_name', StringType(), True), \n",
    "                     StructField('surname', StringType(), True), \n",
    "                     StructField('dob', StringType(), True), \n",
    "                     StructField('city', StringType(), True), \n",
    "                     StructField('email', StringType(), True), \n",
    "                     StructField('group', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_df = spark.createDataFrame(phoenix_df, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_df = right_df.withColumn(\"source_dataset\", f.lit(\"df_r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### READING FROM HIVE INTO A SPARK DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_df = spark.sql(\"SELECT * FROM default.mytable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_df = right_df.withColumn(\"source_dataset\", f.lit(\"df_l\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+----------+--------------+--------------------+-----+--------------+\n",
      "|unique_id|first_name|surname|       dob|          city|               email|group|source_dataset|\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+--------------+\n",
      "|        0|    Julia |   None|2015-10-29|        London| hannah88@powers.com|    0|          df_l|\n",
      "|      105|    Harry |  Tomas|2011-07-30|        Belast|sandra26@anderson...|   21|          df_l|\n",
      "|      115|      None|   Ells|2013-01-20|Stoke-on-Trent|wmcdaniel@nelson.net|   22|          df_l|\n",
      "|      122| Isabella | Wallca|2000-03-24|        London|hilltheresa@pears...|   23|          df_l|\n",
      "|      128|   Edward |   wLis|2005-04-09|          None|whitakernichole@b...|   24|          df_l|\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+--------------+\n",
      "|unique_id|first_name|surname|       dob|          city|               email|group|source_dataset|\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+--------------+\n",
      "|        0|    Julia |   None|2015-10-29|        London| hannah88@powers.com|    0|          df_r|\n",
      "|      105|    Harry |  Tomas|2011-07-30|        Belast|sandra26@anderson...|   21|          df_r|\n",
      "|      115|      None|   Ells|2013-01-20|Stoke-on-Trent|wmcdaniel@nelson.net|   22|          df_r|\n",
      "|      122| Isabella | Wallca|2000-03-24|        London|hilltheresa@pears...|   23|          df_r|\n",
      "|      128|   Edward |   wLis|2005-04-09|          None|whitakernichole@b...|   24|          df_r|\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_df.show(5)\n",
    "right_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Configure splink using the `settings` object\n",
    "\n",
    "Most of `splink` configuration options are stored in a settings dictionary.  This dictionary allows significant customisation, and can therefore get quite complex.  \n",
    "\n",
    "üí• We provide an tool for helping to author valid settings dictionaries, which includes tooltips and autocomplete, which you can find [here](http://robinlinacre.com/splink_settings_editor/).\n",
    "\n",
    "Customisation overrides default values built into splink.  For the purposes of this demo, we will specify a simple settings dictionary, which means we will be relying on these sensible defaults.\n",
    "\n",
    "To help with authoring and validation of the settings dictionary, we have written a [json schema](https://json-schema.org/), which can be found [here](https://github.com/moj-analytical-services/splink/blob/master/splink/files/settings_jsonschema.json).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The comparison expression allows for the case where a first name and surname have been inverted \n",
    "sql_case_expression = \"\"\"\n",
    "CASE \n",
    "WHEN first_name_l = first_name_r AND surname_l = surname_r THEN 4 \n",
    "WHEN first_name_l = surname_r AND surname_l = first_name_r THEN 3\n",
    "WHEN first_name_l = first_name_r THEN 2\n",
    "WHEN surname_l = surname_r THEN 1\n",
    "ELSE 0 \n",
    "END\n",
    "\"\"\"\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"link_only\", \n",
    "    \"max_iterations\": 20,\n",
    "    \"blocking_rules\": [\n",
    "    ],\n",
    "    \"comparison_columns\": [\n",
    "       {\n",
    "            \"custom_name\": \"name_inversion\",\n",
    "            \"custom_columns_used\": [\"first_name\", \"surname\"],\n",
    "            \"case_expression\": sql_case_expression,\n",
    "            \"num_levels\": 5\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"city\",\n",
    "            \"num_levels\": 3\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"email\",\n",
    "            \"num_levels\": 3\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"dob\"\n",
    "        }\n",
    "    ],\n",
    "    \"additional_columns_to_retain\": [\"group\"]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this setting dictionary says:\n",
    "\n",
    "- We are performing a data linking task (the other options are `dedupe_only`, or `link_and_dedupe`)\n",
    "- Rather than generate all possible comparisons (the cartesian product of the input datasets), we are going restrict record comparisons to those generated by at least one of the rules in the specified array\n",
    "- When comparing records, we will use information from the `first_name`, `surname`, `dob`, `city` and `email` columns to compute a match score.\n",
    "- For `first_name` and `surname`, string comparisons will have three levels:\n",
    "    - Level 2: Strings are (almost) exactly the same\n",
    "    - Level 1: Strings are similar \n",
    "    - Level 0: No match\n",
    "- We will make adjustments for term frequencies on the `first_name` and `surname` columns\n",
    "- We will retain the `group` column in the results even though this is not used as part of comparisons.  This is a labelled dataset and `group` contains the true match - i.e. where group matches, the records pertain to the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Estimate match scores using the Expectation Maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.6/site-packages/splink/default_settings.py:175: UserWarning: You have not specified any blocking rules, meaning all comparisons between the input dataset(s) will be generated and blocking will not be used.For large input datasets, this will generally be computationally intractable because it will generate comparisons equal to the number of rows squared.\n",
      "  \"You have not specified any blocking rules, meaning all comparisons between the \"\n",
      "INFO:splink.iterate:Iteration 0 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.3704400897026062 for key name_inversion, level 0\n",
      "INFO:splink.iterate:Iteration 1 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0720943808555603 for key name_inversion, level 4\n",
      "INFO:splink.iterate:Iteration 2 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.013827092945575714 for key dob, level 0\n",
      "INFO:splink.iterate:Iteration 3 complete\n",
      "INFO:splink.model:The maximum change in parameters was 0.0013318657875061035 for key dob, level 1\n",
      "INFO:splink.iterate:Iteration 4 complete\n",
      "INFO:splink.model:The maximum change in parameters was 9.26434401127274e-06 for key dob, level 0\n",
      "INFO:splink.iterate:EM algorithm has converged\n"
     ]
    }
   ],
   "source": [
    "from splink import Splink\n",
    "\n",
    "linker = Splink(settings, [left_df, right_df], spark)\n",
    "df_linked = linker.get_scored_comparisons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_probability</th>\n",
       "      <th>source_dataset_l</th>\n",
       "      <th>unique_id_l</th>\n",
       "      <th>source_dataset_r</th>\n",
       "      <th>unique_id_r</th>\n",
       "      <th>first_name_l</th>\n",
       "      <th>first_name_r</th>\n",
       "      <th>surname_l</th>\n",
       "      <th>surname_r</th>\n",
       "      <th>gamma_name_inversion</th>\n",
       "      <th>city_l</th>\n",
       "      <th>city_r</th>\n",
       "      <th>gamma_city</th>\n",
       "      <th>email_l</th>\n",
       "      <th>email_r</th>\n",
       "      <th>gamma_email</th>\n",
       "      <th>dob_l</th>\n",
       "      <th>dob_r</th>\n",
       "      <th>gamma_dob</th>\n",
       "      <th>group_l</th>\n",
       "      <th>group_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28699</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>792</td>\n",
       "      <td>df_r</td>\n",
       "      <td>738</td>\n",
       "      <td>Aisha</td>\n",
       "      <td>Lottie</td>\n",
       "      <td>King</td>\n",
       "      <td>errFst</td>\n",
       "      <td>0</td>\n",
       "      <td>Telford</td>\n",
       "      <td>Bristol</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>jesus40@allen-graves.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1980-04-24</td>\n",
       "      <td>1986-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>122</td>\n",
       "      <td>df_r</td>\n",
       "      <td>719</td>\n",
       "      <td>Isabella</td>\n",
       "      <td>iilWam</td>\n",
       "      <td>Wallca</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>hilltheresa@pearson.org</td>\n",
       "      <td>taylor70@fisher.nfo</td>\n",
       "      <td>0</td>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>2000-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27306</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>731</td>\n",
       "      <td>df_r</td>\n",
       "      <td>612</td>\n",
       "      <td>Matthew</td>\n",
       "      <td>Caleb</td>\n",
       "      <td>Badyel</td>\n",
       "      <td>Kan</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>Wirangton</td>\n",
       "      <td>0</td>\n",
       "      <td>amandamartinez@melton.com</td>\n",
       "      <td>vmoreno@ussell.zib</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-23</td>\n",
       "      <td>2000-10-09</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3976</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>338</td>\n",
       "      <td>df_r</td>\n",
       "      <td>194</td>\n",
       "      <td>Spencer</td>\n",
       "      <td>acob</td>\n",
       "      <td>Lucas</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>Swansea</td>\n",
       "      <td>Southend-on-Sea</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>barrygary@flores.com</td>\n",
       "      <td>0</td>\n",
       "      <td>2007-10-26</td>\n",
       "      <td>1987-02-06</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27580</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>744</td>\n",
       "      <td>df_r</td>\n",
       "      <td>614</td>\n",
       "      <td>Mcha le</td>\n",
       "      <td>Wright</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>Jude</td>\n",
       "      <td>0</td>\n",
       "      <td>Brighton</td>\n",
       "      <td>Belfast</td>\n",
       "      <td>0</td>\n",
       "      <td>rhondawilliams@gonzalez-scott.com</td>\n",
       "      <td>lynnchapman@crawfard-lozon.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1993-06-23</td>\n",
       "      <td>2017-03-28</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       match_probability source_dataset_l unique_id_l source_dataset_r  \\\n",
       "28699       7.898760e-61             df_l         792             df_r   \n",
       "8401        7.898760e-61             df_l         122             df_r   \n",
       "27306       7.898760e-61             df_l         731             df_r   \n",
       "3976        7.898760e-61             df_l         338             df_r   \n",
       "27580       7.898760e-61             df_l         744             df_r   \n",
       "\n",
       "      unique_id_r first_name_l first_name_r surname_l surname_r  \\\n",
       "28699         738       Aisha       Lottie       King    errFst   \n",
       "8401          719    Isabella       iilWam     Wallca      None   \n",
       "27306         612     Matthew        Caleb     Badyel       Kan   \n",
       "3976          194      Spencer         acob    Lucas       None   \n",
       "27580         614      Mcha le       Wright    Taylor     Jude    \n",
       "\n",
       "       gamma_name_inversion    city_l           city_r  gamma_city  \\\n",
       "28699                     0   Telford          Bristol           0   \n",
       "8401                      0    London             None           0   \n",
       "27306                     0    London        Wirangton           0   \n",
       "3976                      0   Swansea  Southend-on-Sea           0   \n",
       "27580                     0  Brighton          Belfast           0   \n",
       "\n",
       "                                 email_l                         email_r  \\\n",
       "28699                               None        jesus40@allen-graves.com   \n",
       "8401             hilltheresa@pearson.org             taylor70@fisher.nfo   \n",
       "27306          amandamartinez@melton.com              vmoreno@ussell.zib   \n",
       "3976                                None            barrygary@flores.com   \n",
       "27580  rhondawilliams@gonzalez-scott.com  lynnchapman@crawfard-lozon.com   \n",
       "\n",
       "       gamma_email       dob_l       dob_r  gamma_dob group_l group_r  \n",
       "28699            0  1980-04-24  1986-09-22          0     140     128  \n",
       "8401             0  2000-03-24  2000-06-27          0      23     123  \n",
       "27306            0  2017-12-23  2000-10-09          0     126     102  \n",
       "3976             0  2007-10-26  1987-02-06          0      58      34  \n",
       "27580            0  1993-06-23  2017-03-28          0     129     103  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect main dataframe that contains the match scores\n",
    "df_linked.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linked.write.format(\"parquet\").mode(\"overwrite\").saveAsTable('default.matches_spark_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### UPLOADING DATA TO IMPALA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data before insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impala = df_linked.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_probability</th>\n",
       "      <th>source_dataset_l</th>\n",
       "      <th>unique_id_l</th>\n",
       "      <th>source_dataset_r</th>\n",
       "      <th>unique_id_r</th>\n",
       "      <th>first_name_l</th>\n",
       "      <th>first_name_r</th>\n",
       "      <th>surname_l</th>\n",
       "      <th>surname_r</th>\n",
       "      <th>gamma_name_inversion</th>\n",
       "      <th>city_l</th>\n",
       "      <th>city_r</th>\n",
       "      <th>gamma_city</th>\n",
       "      <th>email_l</th>\n",
       "      <th>email_r</th>\n",
       "      <th>gamma_email</th>\n",
       "      <th>dob_l</th>\n",
       "      <th>dob_r</th>\n",
       "      <th>gamma_dob</th>\n",
       "      <th>group_l</th>\n",
       "      <th>group_r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>df_l</td>\n",
       "      <td>0</td>\n",
       "      <td>df_r</td>\n",
       "      <td>0</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Julia</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>2</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>0</td>\n",
       "      <td>df_r</td>\n",
       "      <td>105</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Harry</td>\n",
       "      <td>None</td>\n",
       "      <td>Tomas</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>Belast</td>\n",
       "      <td>0</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>sandra26@anderson-davis.com</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>2011-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>0</td>\n",
       "      <td>df_r</td>\n",
       "      <td>115</td>\n",
       "      <td>Julia</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Ells</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>Stoke-on-Trent</td>\n",
       "      <td>0</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>wmcdaniel@nelson.net</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>2013-01-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.285076e-35</td>\n",
       "      <td>df_l</td>\n",
       "      <td>0</td>\n",
       "      <td>df_r</td>\n",
       "      <td>122</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Isabella</td>\n",
       "      <td>None</td>\n",
       "      <td>Wallca</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>London</td>\n",
       "      <td>2</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>hilltheresa@pearson.org</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>2000-03-24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.898760e-61</td>\n",
       "      <td>df_l</td>\n",
       "      <td>0</td>\n",
       "      <td>df_r</td>\n",
       "      <td>128</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Edward</td>\n",
       "      <td>None</td>\n",
       "      <td>wLis</td>\n",
       "      <td>0</td>\n",
       "      <td>London</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>whitakernichole@booth.com</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>2005-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_probability source_dataset_l unique_id_l source_dataset_r  \\\n",
       "0       1.000000e+00             df_l           0             df_r   \n",
       "1       7.898760e-61             df_l           0             df_r   \n",
       "2       7.898760e-61             df_l           0             df_r   \n",
       "3       8.285076e-35             df_l           0             df_r   \n",
       "4       7.898760e-61             df_l           0             df_r   \n",
       "\n",
       "  unique_id_r first_name_l first_name_r surname_l surname_r  \\\n",
       "0           0       Julia        Julia       None      None   \n",
       "1         105       Julia        Harry       None     Tomas   \n",
       "2         115       Julia          None      None      Ells   \n",
       "3         122       Julia     Isabella       None    Wallca   \n",
       "4         128       Julia       Edward       None      wLis   \n",
       "\n",
       "   gamma_name_inversion  city_l          city_r  gamma_city  \\\n",
       "0                     4  London          London           2   \n",
       "1                     0  London          Belast           0   \n",
       "2                     0  London  Stoke-on-Trent           0   \n",
       "3                     0  London          London           2   \n",
       "4                     0  London            None           0   \n",
       "\n",
       "               email_l                      email_r  gamma_email       dob_l  \\\n",
       "0  hannah88@powers.com          hannah88@powers.com            2  2015-10-29   \n",
       "1  hannah88@powers.com  sandra26@anderson-davis.com            0  2015-10-29   \n",
       "2  hannah88@powers.com         wmcdaniel@nelson.net            0  2015-10-29   \n",
       "3  hannah88@powers.com      hilltheresa@pearson.org            0  2015-10-29   \n",
       "4  hannah88@powers.com    whitakernichole@booth.com            0  2015-10-29   \n",
       "\n",
       "        dob_r  gamma_dob group_l group_r  \n",
       "0  2015-10-29          1       0       0  \n",
       "1  2011-07-30          0       0      21  \n",
       "2  2013-01-20          0       0      22  \n",
       "3  2000-03-24          0       0      23  \n",
       "4  2005-04-09          0       0      24  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_impala.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We round match probabilities and pick entities that are a match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impala = df_impala.round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impala = df_impala[df_impala[\"match_probability\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering duplicate columns\n",
    "df_impala = df_impala.filter(regex=\"_l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_impala.columns = df_impala.columns.str.replace(\"_l\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### We are ready to upsert our matches into Impala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jaydebeapi\n",
    "conn = jaydebeapi.connect(\"com.cloudera.impala.jdbc.DataSource\",\n",
    "                          \"jdbc:impala://\"+os.environ[\"IMPALA_HOST\"]+\":443/;ssl=1;transportMode=http;httpPath=cliservice;AuthMech=3;\",\n",
    "                          {'UID': os.environ[\"WORKLOAD_USER\"], 'PWD': os.environ[\"WORKLOAD_PASSWORD\"]},\n",
    "                          '/home/cdsw/impala_drivers/ImpalaJDBC41.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS ER_MATCHES (\n",
    "        unique_id VARCHAR,\n",
    "        first_name VARCHAR,\n",
    "        surname VARCHAR,\n",
    "        dob VARCHAR,\n",
    "        city  VARCHAR,\n",
    "        email VARCHAR,\n",
    "        groupP VARCHAR)\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_linkage(data):\n",
    "\n",
    "    sql = \"\"\"insert into ER_MATCHES \\\n",
    "         (unique_id ,first_name,surname,dob,city,email,groupP) \\\n",
    "         values (?,?,?,?,?,?,?)\"\"\"\n",
    "    #print(data)\n",
    "    cursor.executemany(sql,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_data_jaydebeapi(data, records=100):\n",
    "    total_records=0\n",
    "    header = True\n",
    "    rows = []\n",
    "    i=1\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        rows.append ([f\"{row['unique_id']}\",\\\n",
    "                  f\"{row['first_name']}\",f\"{row['surname']}\",\\\n",
    "                  f\"{row['dob']}\",f\"{row['city']}\", \\\n",
    "                  f\"{row['email']}\", f\"{row['group']}\"])\n",
    "        total_records=total_records+1\n",
    "\n",
    "        if i < records + 1 :   \n",
    "            i=i+1\n",
    "        else :\n",
    "            upsert_linkage(rows)\n",
    "            rows = []\n",
    "            i=1\n",
    "            print (f\"Ingested {total_records} records\")\n",
    "\n",
    "    if len(rows) > 0 :\n",
    "        upsert_linkage(rows)\n",
    "\n",
    "    print (f\"Ingested {total_records} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 101 records\n",
      "Ingested 181 records\n"
     ]
    }
   ],
   "source": [
    "upsert_data_jaydebeapi(df_impala)\n",
    "\n",
    "#curs.fetchall()\n",
    "\n",
    "#cursor.close()\n",
    "#conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Custom Atlas Type (Process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to instantiate the connection to Atlas in CDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atlasclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endpoint, Username and Passoword are stored as CML project variables and passed dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlasclient.client import Atlas\n",
    "client = Atlas(os.environ[\"ATLAS_ENDPOINT\"], port='', username=os.environ[\"ATLAS_USER\"], password=os.environ[\"ATLAS_PASSWORD\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have successfully established the connection. Next we can create a custom Atlas type (process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef_dict = {\n",
    "    \"enumTypes\": [],\n",
    "    \"structTypes\": [],\n",
    "    \"classificationDefs\":[],\n",
    "    \"entityDefs\": [{\n",
    "        \"superTypes\": [\"Process\"],\n",
    "        \"name\": \"ER_algorithm\",\n",
    "        \"description\":\"custom_type_for_Entity_Resolution\",\n",
    "        \"attributeDefs\": [{\n",
    "            \"name\": \"startTime\",\n",
    "            \"isOptional\": True,\n",
    "            \"isUnique\": False,\n",
    "            \"isIndexable\": False,\n",
    "            \"typeName\":\"string\",\n",
    "            \"valuesMaxCount\":1,\n",
    "            \"cardinality\":\"SINGLE\",\n",
    "            \"valuesMinCount\":0\n",
    "        }]\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now register the new type with Atlas. For more on the Atlas type model, please visit this page: https://docs.cloudera.com/runtime/7.2.7/cdp-governance-overview/topics/atlas-metadata-model-overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<atlasclient.models.TypeDefs at 0x7fe148bb3780>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Has already run once so will not run again\n",
    "client.typedefs.create(data=typedef_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Instantiate the EM algorithm in Atlas along with lineage reflecting our Linkage Job above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: we need to pass the Atlas guid for the two datasets we compared above as they were registered in Atlas when they were stored as a Spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving GUID's for the three tables via Atlas Client - search by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420997b4-e1d2-4b88-ab46-6c7b794cf67d\n",
      "{'owner': 'hbase', 'createTime': 1617763073453, 'qualifiedName': 'hbase:acl@cm', 'name': 'hbase:acl', 'description': 'hbase:acl'}\n",
      "hbase_table\n",
      "{'owner': 'hbase', 'createTime': 1617763073453, 'qualifiedName': 'hbase:acl@cm', 'name': 'hbase:acl', 'description': 'hbase:acl'}\n",
      "f234866f-4396-4557-b23a-66ed3a7e82b1\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927034944, 'qualifiedName': 'SYSTEM:SEQUENCE@cm', 'name': 'SYSTEM:SEQUENCE', 'description': 'SYSTEM:SEQUENCE'}\n",
      "hbase_table\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927034944, 'qualifiedName': 'SYSTEM:SEQUENCE@cm', 'name': 'SYSTEM:SEQUENCE', 'description': 'SYSTEM:SEQUENCE'}\n",
      "94de73a3-137d-4532-8634-9bf4060b61c7\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927025330, 'qualifiedName': 'SYSTEM:CATALOG@cm', 'name': 'SYSTEM:CATALOG', 'description': 'SYSTEM:CATALOG'}\n",
      "hbase_table\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927025330, 'qualifiedName': 'SYSTEM:CATALOG@cm', 'name': 'SYSTEM:CATALOG', 'description': 'SYSTEM:CATALOG'}\n",
      "7d335a07-8e16-4a27-815c-66825ea5939b\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927071905, 'qualifiedName': 'default:CML_WORKSHOP_TABLE_RIGHT@cm', 'name': 'CML_WORKSHOP_TABLE_RIGHT', 'description': 'CML_WORKSHOP_TABLE_RIGHT'}\n",
      "hbase_table\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927071905, 'qualifiedName': 'default:CML_WORKSHOP_TABLE_RIGHT@cm', 'name': 'CML_WORKSHOP_TABLE_RIGHT', 'description': 'CML_WORKSHOP_TABLE_RIGHT'}\n",
      "acd99fe6-96f1-4659-92de-9a8c40a516a3\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927068334, 'qualifiedName': 'SYSTEM:MUTEX@cm', 'name': 'SYSTEM:MUTEX', 'description': 'SYSTEM:MUTEX'}\n",
      "hbase_table\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927068334, 'qualifiedName': 'SYSTEM:MUTEX@cm', 'name': 'SYSTEM:MUTEX', 'description': 'SYSTEM:MUTEX'}\n",
      "d27b856d-f31f-463c-a5a4-d67d9be7eba9\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927049852, 'qualifiedName': 'SYSTEM:FUNCTION@cm', 'name': 'SYSTEM:FUNCTION', 'description': 'SYSTEM:FUNCTION'}\n",
      "hbase_table\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927049852, 'qualifiedName': 'SYSTEM:FUNCTION@cm', 'name': 'SYSTEM:FUNCTION', 'description': 'SYSTEM:FUNCTION'}\n",
      "254a3983-c44b-470b-9140-692d9556e38b\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927043210, 'qualifiedName': 'SYSTEM:STATS@cm', 'name': 'SYSTEM:STATS', 'description': 'SYSTEM:STATS'}\n",
      "hbase_table\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927043210, 'qualifiedName': 'SYSTEM:STATS@cm', 'name': 'SYSTEM:STATS', 'description': 'SYSTEM:STATS'}\n",
      "d8dc063d-988a-4a5d-a52b-08a402a90006\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927055650, 'qualifiedName': 'SYSTEM:LOG@cm', 'name': 'SYSTEM:LOG', 'description': 'SYSTEM:LOG'}\n",
      "hbase_table\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927055650, 'qualifiedName': 'SYSTEM:LOG@cm', 'name': 'SYSTEM:LOG', 'description': 'SYSTEM:LOG'}\n"
     ]
    }
   ],
   "source": [
    "params = {'typeName': 'hbase_table', 'attrName': 'data', 'attrValue': 'provider','offset': '1', 'limit':'100'}\n",
    "search_results = client.search_basic(**params)\n",
    "for s in search_results:\n",
    "    for e in s.entities:\n",
    "        print(e.guid)\n",
    "        print(e.attributes)\n",
    "        #print(e.attributes.values)\n",
    "        print(e.typeName)\n",
    "        print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'typeName': 'hive_table', 'attrName': 'name', 'attrValue': 'cc_data', 'offset': '1', 'limit':'10'}\n",
    "search_results = client.search_attribute(**params)\n",
    "for s in search_results:\n",
    "    for e in s.entities:\n",
    "        print(e.guid)\n",
    "        print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "for s in search_results:\n",
    "    print(s.entities.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1ad7307-47d9-4972-a64c-d415d15f5231\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927521000, 'qualifiedName': 'default.cml_workshop_table_right@cm', 'name': 'cml_workshop_table_right'}\n",
      "1a1d46fd-5bb2-42bb-b96f-146f1b740a97\n",
      "{'owner': 'hive', 'createTime': 1617927563000, 'qualifiedName': 'default.tmp_compactor_cml_workshop_table_right_1617927561339_temp-89ec323a-9b23-4477-a721-e7247f5959c7@cm', 'name': 'tmp_compactor_cml_workshop_table_right_1617927561339'}\n",
      "fcfbb0e5-2a8f-49e6-bf10-46b6dee8398a\n",
      "{'owner': 'hive', 'createTime': 1617927875000, 'qualifiedName': 'default.tmp_compactor_cml_workshop_table_right_1617927874984_temp-89ec323a-9b23-4477-a721-e7247f5959c7@cm', 'name': 'tmp_compactor_cml_workshop_table_right_1617927874984'}\n",
      "18c03667-05a0-4485-9553-097ac8ad2be9\n",
      "{'owner': 'pauldefusco', 'createTime': 1617927954000, 'qualifiedName': 'default.customer_interactions_cicd@cm', 'name': 'customer_interactions_cicd'}\n",
      "53c8b9b6-8bea-4fcd-9b37-6961a1ed11e8\n",
      "{'owner': 'pauldefusco', 'createTime': 1617928014000, 'qualifiedName': 'default.matches_spark_table@cm', 'name': 'matches_spark_table'}\n",
      "e3e19d9f-2027-4142-b72d-ae77c6415601\n",
      "{'owner': 'pauldefusco', 'createTime': 1617928017000, 'qualifiedName': 'default.er_matches@cm', 'name': 'er_matches'}\n"
     ]
    }
   ],
   "source": [
    "data = {'typeName': 'hive_table', 'attrName': 'name', 'attrValue': 'cc_data', 'offset': '1', 'limit': '100'}\n",
    "search_results = client.search_basic.create(data=data)\n",
    "for e in search_results.entities:\n",
    "    print(e.guid)\n",
    "    print(e.attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can now create a new process type in Atlas reflecting the ER Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Before doing so, open Atlas and search for the tables you created. Then copy and paste the GUID for each entity down in the dictionary below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT SCREENSHOT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_entity_dict = {\n",
    "  \"entity\" : {\n",
    "    \"guid\" : \"-2089428075574333\",\n",
    "    \"status\" : \"ACTIVE\",\n",
    "    \"createdBy\" : \"pdefusco\",\n",
    "    \"updatedBy\" : \"pdefusco\",\n",
    "    \"createTime\" : \"12342\",\n",
    "    \"updateTime\" : \"12342\",\n",
    "    \"version\" : \"12342\",\n",
    "    \"relationshipAttributes\" : {},\n",
    "    \"classifications\" : [],\n",
    "    \"typeName\" : \"ER_algorithm\",\n",
    "    \"attributes\" : {\n",
    "      \"startTime\" : \"123\",\n",
    "      \"qualifiedName\": \"EM Record Linkage\",\n",
    "      \"name\":\"EM Record Linkage\",\n",
    "      \"description\":\"Record Linkage Algorithm\",\n",
    "      \"owner\": \"pdefusco\",\n",
    "        #, \n",
    "      \"inputs\":[{\"guid\": \"2bcde123-67f2-4ed7-a781-4824085a2d90\", \"typeName\":\"hive_table\"},\n",
    "               {\"guid\": \"e26109c4-b70a-4583-a361-3de4539f532f\", \"typeName\":\"hbase_table\"}], \n",
    "      \"outputs\":[{\"guid\":\"ae778e6b-57f4-486e-9643-7ae574a9e16b\", \"type_name\":\"hive_table\"}]\n",
    "    }, \n",
    "  },\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "HTTP request failed for POST https://mlworkshop-04212021-master0.mlworksh.z30z-14kp.cloudera.site:31443/api/atlas/v2/entity: Not found 404: {\"errorCode\":\"ATLAS-404-00-00A\",\"errorMessage\":\"Referenced entity ae778e6b-57f4-486e-9643-7ae574a9e16b is not found\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-1d9d0a7f4b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity_post\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess_entity_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/atlasclient/events.py\u001b[0m in \u001b[0;36mreplacement\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mpublish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTARTED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mpublish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAILED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/atlasclient/models.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mUpdate\u001b[0m \u001b[0ma\u001b[0m \u001b[0mresource\u001b[0m \u001b[0mby\u001b[0m \u001b[0mpassing\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodifications\u001b[0m \u001b[0mvia\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0marguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \"\"\"\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/atlasclient/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content_type, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# any error responses will generate exceptions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mhandle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mLOG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Response headers: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/atlasclient/exceptions.py\u001b[0m in \u001b[0;36mhandle_response\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'retry_after'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'retry-after'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNotFound\u001b[0m: HTTP request failed for POST https://mlworkshop-04212021-master0.mlworksh.z30z-14kp.cloudera.site:31443/api/atlas/v2/entity: Not found 404: {\"errorCode\":\"ATLAS-404-00-00A\",\"errorMessage\":\"Referenced entity ae778e6b-57f4-486e-9643-7ae574a9e16b is not found\"}"
     ]
    }
   ],
   "source": [
    "client.entity_post.create(data=process_entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Go back to the CDP Homepage, then Data Catalog -> Atlas and browse for \"ER_algorithm\". Open the instance and navigate to the \"Lineage\" tab "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/EM_Record_Linkage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create a new Atlas Process Type related to writing a Spark Dataset to Impala and Instantiate it with source and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef_dict = {\n",
    "    \"enumTypes\": [],\n",
    "    \"structTypes\": [],\n",
    "    \"classificationDefs\":[],\n",
    "    \"entityDefs\": [{\n",
    "        \"superTypes\": [\"Process\"],\n",
    "        \"name\": \"Write_to_Impala\",\n",
    "        \"description\":\"write_to_impala\",\n",
    "        \"attributeDefs\": [{\n",
    "            \"name\": \"startTime\",\n",
    "            \"isOptional\": True,\n",
    "            \"isUnique\": False,\n",
    "            \"isIndexable\": False,\n",
    "            \"typeName\":\"string\",\n",
    "            \"valuesMaxCount\":1,\n",
    "            \"cardinality\":\"SINGLE\",\n",
    "            \"valuesMinCount\":0\n",
    "        }]\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_entity_dict = {\n",
    "  \"entity\" : {\n",
    "    \"guid\" : \"-2089428075574888\",\n",
    "    \"status\" : \"ACTIVE\",\n",
    "    \"createdBy\" : \"pdefusco\",\n",
    "    \"updatedBy\" : \"pdefusco\",\n",
    "    \"createTime\" : \"12342\",\n",
    "    \"updateTime\" : \"12342\",\n",
    "    \"version\" : \"12342\",\n",
    "    \"relationshipAttributes\" : {},\n",
    "    \"classifications\" : [],\n",
    "    \"typeName\" : \"Write_to_Impala\",\n",
    "    \"attributes\" : {\n",
    "      \"startTime\" : \"123\",\n",
    "      \"qualifiedName\": \"Write_to_Impala\",\n",
    "      \"name\":\"Write_to_Impala\",\n",
    "      \"description\":\"Record Linkage Algorithm\",\n",
    "      \"owner\": \"pdefusco\",\n",
    "        #, \n",
    "      \"inputs\":[{\"guid\": \"ae778e6b-57f4-486e-9643-7ae574a9e16b\", \"typeName\":\"hive_table\"}], \n",
    "      \"outputs\":[{\"guid\":\"INSERT IMPALA TABLE GUID HERE\", \"type_name\":\"IMPALA_TABLE???\"}]\n",
    "    }, \n",
    "  },\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/ER_atlas_lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can optionally remove the EM Algorithm instance from Atlas via the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = client.entity_guid(\"44848fe5-6950-4a73-a89c-9775b736b4c9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity.entity['attributes'][\"owner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have completed our introduction to Splink and the Atlas Client. \n",
    "## Next we will simulate a real world Application with CML Jobs and COD (Cloudera Operational Database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
