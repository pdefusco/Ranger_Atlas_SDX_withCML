{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to End Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Imports and setup\n",
    "\n",
    "The following is just boilerplate code that sets up the Spark session and sets some other non-essential configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct Cloud Storage URL is: s3a://demo-aws-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "\n",
    "#Extracting the correct URL from hive-site.xml\n",
    "tree = ET.parse('/etc/hadoop/conf/hive-site.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for prop in root.findall('property'):\n",
    "    if prop.find('name').text == \"hive.metastore.warehouse.dir\":\n",
    "        storage = prop.find('value').text.split(\"/\")[0] + \"//\" + prop.find('value').text.split(\"/\")[2]\n",
    "\n",
    "print(\"The correct Cloud Storage URL is: {}\".format(storage))\n",
    "\n",
    "os.environ['STORAGE'] = storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf=SparkConf()\n",
    "\n",
    "# Load in a jar that provides extended string comparison functions such as Jaro Winkler.\n",
    "# Splink\n",
    "#     conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#     conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11')\n",
    "\n",
    "#sc = SparkContext.getOrCreate(conf=conf)\n",
    "#sc.setCheckpointDir(\"temp_graphframes/\")\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Entity Resolution with Lineage\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-1\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ['STORAGE'])\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .config(\"spark.jars\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register UDFs\n",
    "from pyspark.sql import types\n",
    "spark.udf.registerJavaFunction('jaro_winkler_sim', 'uk.gov.moj.dash.linkage.JaroWinklerSimilarity', types.DoubleType())\n",
    "spark.udf.registerJavaFunction('Dmetaphone', 'uk.gov.moj.dash.linkage.DoubleMetaphone', types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://100.100.186.114:20050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5.7.2.2.0-244</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://172.20.0.1:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Entity Resolution with Lineage</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa7b5c32160>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig()  # Means logs will print in Jupyter Lab\n",
    "\n",
    "# Set to DEBUG if you want splink to log the SQL statements it's executing under the hood\n",
    "logging.getLogger(\"splink\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read in the data\n",
    "\n",
    "The `l` and `r` stand for 'left' and 'right.  It doesn't matter which of the two datasets you choose as the left, performance and results will be the same.\n",
    "\n",
    "‚ö†Ô∏è Note that `splink` makes the following assumptions about your data:\n",
    "\n",
    "-  There is a field containing a unique record identifier in each dataset\n",
    "-  The two datasets being linked have common column names - e.g. date of birth is represented in both datasets in a field of the same name.   In many cases, this means that the user needs to rename columns prior to using `splink`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### READING FROM PHOENIX INTO A SPARK DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Db:\n",
    "    def __init__(self):\n",
    "        opts = {}\n",
    "        opts['authentication'] = 'BASIC'\n",
    "        opts['avatica_user'] = os.environ[\"WORKLOAD_USER\"]\n",
    "        opts['avatica_password'] = os.environ[\"WORKLOAD_PASSWORD\"]\n",
    "        database_url = os.environ[\"OPDB_ENDPOINT_AWS2\"]\n",
    "        self.TABLENAME = \"test_table_paul\"\n",
    "        self.conn = phoenixdb.connect(database_url, autocommit=True,**opts)\n",
    "        self.curs = self.conn.cursor()\n",
    "        \n",
    "    def get_data(self):\n",
    "\n",
    "        query = f\"SELECT * FROM CML_WORKSHOP_TABLE_RIGHT\"\n",
    "\n",
    "        model.curs.execute(query)\n",
    "        rows = model.curs.fetchall()\n",
    "\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig( level=logging.DEBUG)\n",
    "\n",
    "import os\n",
    "import phoenixdb\n",
    "model = Db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_df = model.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('unique_id', StringType(), True),\n",
    "                     StructField('first_name', StringType(), True), \n",
    "                     StructField('surname', StringType(), True), \n",
    "                     StructField('dob', StringType(), True), \n",
    "                     StructField('city', StringType(), True), \n",
    "                     StructField('email', StringType(), True), \n",
    "                     StructField('group', StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_df = spark.createDataFrame(phoenix_df, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### READING FROM HIVE INTO A SPARK DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_df = spark.sql(\"SELECT * FROM default.mytable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+----------+------+--------------------+-----+\n",
      "|unique_id|first_name|surname|       dob|  city|               email|group|\n",
      "+---------+----------+-------+----------+------+--------------------+-----+\n",
      "|        1|    Julia | Taylor|2015-07-31|London| hannah88@powers.com|    0|\n",
      "|        2|    Julia | Taylor|2016-01-27|London| hannah88@powers.com|    0|\n",
      "|        3|    Julia | Taylor|2015-10-29|  null|  hannah88opowersc@m|    0|\n",
      "|        5|     Noah | Watson|2008-03-23|Bolton|matthew78@ballard...|    1|\n",
      "|        6|    Watson|  Noah |2008-03-23|  null|matthew78@ballard...|    1|\n",
      "+---------+----------+-------+----------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+\n",
      "|unique_id|first_name|surname|       dob|          city|               email|group|\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+\n",
      "|        0|    Julia |   None|2015-10-29|        London| hannah88@powers.com|    0|\n",
      "|      105|    Harry |  Tomas|2011-07-30|        Belast|sandra26@anderson...|   21|\n",
      "|      115|      None|   Ells|2013-01-20|Stoke-on-Trent|wmcdaniel@nelson.net|   22|\n",
      "|      122| Isabella | Wallca|2000-03-24|        London|hilltheresa@pears...|   23|\n",
      "|      128|   Edward |   wLis|2005-04-09|          None|whitakernichole@b...|   24|\n",
      "+---------+----------+-------+----------+--------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "left_df.show(5)\n",
    "right_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Configure splink using the `settings` object\n",
    "\n",
    "Most of `splink` configuration options are stored in a settings dictionary.  This dictionary allows significant customisation, and can therefore get quite complex.  \n",
    "\n",
    "üí• We provide an tool for helping to author valid settings dictionaries, which includes tooltips and autocomplete, which you can find [here](http://robinlinacre.com/splink_settings_editor/).\n",
    "\n",
    "Customisation overrides default values built into splink.  For the purposes of this demo, we will specify a simple settings dictionary, which means we will be relying on these sensible defaults.\n",
    "\n",
    "To help with authoring and validation of the settings dictionary, we have written a [json schema](https://json-schema.org/), which can be found [here](https://github.com/moj-analytical-services/splink/blob/master/splink/files/settings_jsonschema.json).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"link_type\": \"link_only\", \n",
    "    \"max_iterations\": 5,\n",
    "    \"blocking_rules\": [\n",
    "        'l.first_name = r.first_name',\n",
    "        'l.surname = r.surname',\n",
    "        'l.dob = r.dob'\n",
    "    ],\n",
    "    \"comparison_columns\": [\n",
    "       {\n",
    "        \"custom_name\": \"name_inversion\",\n",
    "        \"custom_columns_used\": [\"first_name\", \"surname\", \"dob\"],\n",
    "        \"case_expression\": \"CASE WHEN first_name_l = first_name_r AND surname_l = surname_r THEN 2 WHEN first_name_l = surname_r AND surname_l = first_name_r THEN 1 ELSE 0 END\",\n",
    "        \"num_levels\": 3\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"dob\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"city\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"email\"\n",
    "        }\n",
    "    ],\n",
    "    \"additional_columns_to_retain\": [\"group\"]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this setting dictionary says:\n",
    "\n",
    "- We are performing a data linking task (the other options are `dedupe_only`, or `link_and_dedupe`)\n",
    "- Rather than generate all possible comparisons (the cartesian product of the input datasets), we are going restrict record comparisons to those generated by at least one of the rules in the specified array\n",
    "- When comparing records, we will use information from the `first_name`, `surname`, `dob`, `city` and `email` columns to compute a match score.\n",
    "- For `first_name` and `surname`, string comparisons will have three levels:\n",
    "    - Level 2: Strings are (almost) exactly the same\n",
    "    - Level 1: Strings are similar \n",
    "    - Level 0: No match\n",
    "- We will make adjustments for term frequencies on the `first_name` and `surname` columns\n",
    "- We will retain the `group` column in the results even though this is not used as part of comparisons.  This is a labelled dataset and `group` contains the true match - i.e. where group matches, the records pertain to the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Estimate match scores using the Expectation Maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "missing a required argument: 'spark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8ae291ab81d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msplink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSplink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlinker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mright_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtarget_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scored_comparisons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CallMemo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_localns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m         \u001b[0mcheck_argument_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/cdsw/.local/lib/python3.6/site-packages/typeguard/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, frame_locals, args, kwargs, forward_refs_policy)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mframe_locals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'frame must be specified if args or kwargs is None'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2995\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mcan\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2996\u001b[0m         \"\"\"\n\u001b[0;32m-> 2997\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbind_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36m_bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   2910\u001b[0m                             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'missing a required argument: {arg!r}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m                             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m                 \u001b[0;31m# We have a positional argument to process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: missing a required argument: 'spark'"
     ]
    }
   ],
   "source": [
    "from splink import Splink\n",
    "\n",
    "linker = Splink(settings, spark, df_l=left_df, df_r=right_df)\n",
    "target_df = linker.get_scored_comparisons()\n",
    "\n",
    "# Later, we will make term frequency adjustments.  \n",
    "# Persist caches these results in memory, preventing them having to be recomputed when we make these adjustments.\n",
    "target_df.persist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect main dataframe that contains the match scores\n",
    "target_df.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jaydebeapi\n",
    "conn = jaydebeapi.connect(\"com.cloudera.impala.jdbc.DataSource\",\n",
    "                          \"jdbc:impala://\"+os.environ[\"IMPALA_HOST\"]+\":443/;ssl=1;transportMode=http;httpPath=cliservice;AuthMech=3;\",\n",
    "                          {'UID': os.environ[\"WORKLOAD_USER\"], 'PWD': os.environ[\"WORKLOAD_PASSWORD\"]},\n",
    "                          '/home/cdsw/impala_drivers/ImpalaJDBC41.jar')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "upsert_data_jaydebeapi(df, records=100)\n",
    "\n",
    "#curs.fetchall()\n",
    "\n",
    "curs.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.write.format('parquet').mode(\"overwrite\").saveAsTable('target_ER_TABLE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect results \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `params` property of the `linker` is an object that contains a lot of diagnostic information about how the match probability was computed.  The following cells demonstrate some of its functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative representation of the parameters displays them in terms of the effect different values in the comparison vectors have on the match probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.bayes_factor_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If charts aren't displaying correctly in your notebook, you can write them to a file (by default splink_charts.html)\n",
    "params.all_charts_write_html_file(\"splink_charts.html\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also generate a report which explains how the match probability was computed for an individual comparison row.  \n",
    "\n",
    "Note that you need to convert the row to a dictionary for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial probability of match (prior) = Œª = 0.3851017951965332\n",
      "\n",
      "Comparison of name_inversion.  Values are:\n",
      "name_inversion_l: Patel, Mohammed , 1989-03-19\n",
      "name_inversion_r: Mohammed , Patel, 1989-03-19\n",
      "Comparison has 3 levels\n",
      "ùõæ for this comparison = gamma_name_inversion = 1\n",
      "Amongst matches, m = P(ùõæ|match) = 0.12372622638940811\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 2.1532594018935924e-06\n",
      "Bayes factor = m/u = 57459.97267240647\n",
      "New probability of match (updated belief): 0.9999722124537389\n",
      "\n",
      "Comparison of dob.  Values are:\n",
      "dob_l: 1989-03-19\n",
      "dob_r: 1989-03-19\n",
      "Comparison has 2 levels\n",
      "ùõæ for this comparison = gamma_dob = 1\n",
      "Amongst matches, m = P(ùõæ|match) = 0.856454074382782\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 0.048313207924366\n",
      "Bayes factor = m/u = 17.727120826328797\n",
      "New probability of match (updated belief): 0.9999984324428572\n",
      "\n",
      "Comparison of city.  Values are:\n",
      "city_l: Sheffield\n",
      "city_r: Sheffield\n",
      "Comparison has 2 levels\n",
      "ùõæ for this comparison = gamma_city = 1\n",
      "Amongst matches, m = P(ùõæ|match) = 0.78189617395401\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 0.17813900113105774\n",
      "Bayes factor = m/u = 4.38924754820403\n",
      "New probability of match (updated belief): 0.9999996428638341\n",
      "\n",
      "Comparison of email.  Values are:\n",
      "email_l: None\n",
      "email_r: heather31@cabrera-reed.com\n",
      "Comparison has 2 levels\n",
      "ùõæ for this comparison = gamma_email = -1\n",
      "Amongst matches, m = P(ùõæ|match) = 1.0\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 1.0\n",
      "Bayes factor = m/u = 1.0\n",
      "New probability of match (updated belief): 0.9999996428638341\n",
      "\n",
      "Final probability of match = 0.9999996428638341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from splink.intuition import intuition_report\n",
    "row_dict = df_e.toPandas().sample(1).to_dict(orient=\"records\")[0]\n",
    "print(intuition_report(row_dict, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from splink.diagnostics import splink_score_histogram\n",
    "from pyspark.sql.functions import expr \n",
    "splink_score_histogram(df_e.filter(expr('match_probability > 0.001')), spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Custom Atlas Type (Process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to instantiate the connection to Atlas in CDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atlasclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endpoint, Username and Passoword are stored as CML project variables and passed dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlasclient.client import Atlas\n",
    "client = Atlas(os.environ[\"atlas_endpoint\"], port='', username=os.environ[\"atlas_username\"], password=os.environ[\"atlas_password\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Client connection is working by querying a random Atlas entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "guid = \"c845eb62-d85d-4591-8abe-0c31449cdd95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = client.entity_guid(guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:atlasclient.base:Missing attr EntityGuid: searchParameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'createTime': 1607119259591,\n",
       " 'description': None,\n",
       " 'displayName': None,\n",
       " 'metadata': {'default_project_engine_type': 'legacy_engine',\n",
       "  'project_description': 'Build an XGBoost model to predict churn using customer telco data.',\n",
       "  'project_visibility': 'private',\n",
       "  'service_name': 'mlgov/mlgovapiserver.mlx.cloudera.site@DEMO-AWS.YLCU-ATMI.CLOUDERA.SITE',\n",
       "  'session_username': 'alexbleakley'},\n",
       " 'modifiedTime': 1607119259591,\n",
       " 'name': 'Churn Modeling with XGBoost - alexbleakley',\n",
       " 'owner': 'alexbleakley',\n",
       " 'qualifiedName': 'crn:cdp:ml:us-west-1:8a1e15cd-04c2-48aa-8f35-b4a8c11997d3:workspace:ab34ab61-368e-46a2-923e-6c5830585734/affaa215-c794-436c-af69-dcccfffb3d02',\n",
       " 'replicatedFrom': None,\n",
       " 'replicatedTo': None,\n",
       " 'userDescription': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.entity['attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have successfully established the connection. Next we can create a custom Atlas type (process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef_dict = {\n",
    "    \"enumTypes\": [],\n",
    "    \"structTypes\": [],\n",
    "    \"classificationDefs\":[],\n",
    "    \"entityDefs\": [{\n",
    "        \"superTypes\": [\"Process\"],\n",
    "        \"name\": \"EM_algorithm_linkage\",\n",
    "        \"description\":\"custom_type_for_Entity_Resolution\",\n",
    "        \"attributeDefs\": [{\n",
    "            \"name\": \"startTime\",\n",
    "            \"isOptional\": True,\n",
    "            \"isUnique\": False,\n",
    "            \"isIndexable\": False,\n",
    "            \"typeName\":\"string\",\n",
    "            \"valuesMaxCount\":1,\n",
    "            \"cardinality\":\"SINGLE\",\n",
    "            \"valuesMinCount\":0\n",
    "        }]\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now register the new type with Atlas. For more on the Atlas type model, please visit this page: https://docs.cloudera.com/runtime/7.2.7/cdp-governance-overview/topics/atlas-metadata-model-overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Has already run once so will not run again\n",
    "#client.typedefs.create(data=typedef_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Instantiate the EM algorithm in Atlas along with lineage reflecting our Linkage Job above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: we need to pass the Atlas guid for the two datasets we compared above as they were registered in Atlas when they were stored as a Spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_entity_dict = {\n",
    "  \"entity\" : {\n",
    "    \"guid\" : \"-2089428075574333\",\n",
    "    \"status\" : \"ACTIVE\",\n",
    "    \"createdBy\" : \"pdefusco\",\n",
    "    \"updatedBy\" : \"pdefusco\",\n",
    "    \"createTime\" : \"12342\",\n",
    "    \"updateTime\" : \"12342\",\n",
    "    \"version\" : \"12342\",\n",
    "    \"relationshipAttributes\" : {},\n",
    "    \"classifications\" : [],\n",
    "    \"typeName\" : \"EM_algorithm_linkage\",\n",
    "    \"attributes\" : {\n",
    "      \"startTime\" : \"123\",\n",
    "      \"qualifiedName\": \"EM Record Linkage\",\n",
    "      \"name\":\"EM Record Linkage\",\n",
    "      \"description\":\"Record Linkage Algorithm\",\n",
    "      \"owner\": \"pdefusco\",\n",
    "        #, \n",
    "      \"inputs\":[{\"guid\": \"aa955089-5a11-46d9-9dbf-2f6b75f4d65b\", \"typeName\":\"hive_table\"},\n",
    "               {\"guid\": \"43d788ce-4af4-4253-af0b-465ea45c1b93\", \"typeName\":\"hive_table\"}], \n",
    "      \"outputs\":[{\"guid\":\"ac1bdcb3-73c8-4198-a8e6-0aa104c606bb\", \"type_name\":\"hive_table\"}]\n",
    "    }, \n",
    "  },\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guidAssignments': {'-2089428075574333': '44848fe5-6950-4a73-a89c-9775b736b4c9'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.entity_post.create(data=process_entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create a new Atlas Process Type related to writing a Spark Dataset to Impala and Instantiate it with source and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef_dict = {\n",
    "    \"enumTypes\": [],\n",
    "    \"structTypes\": [],\n",
    "    \"classificationDefs\":[],\n",
    "    \"entityDefs\": [{\n",
    "        \"superTypes\": [\"Process\"],\n",
    "        \"name\": \"Write_to_Impala\",\n",
    "        \"description\":\"write_to_impala\",\n",
    "        \"attributeDefs\": [{\n",
    "            \"name\": \"startTime\",\n",
    "            \"isOptional\": True,\n",
    "            \"isUnique\": False,\n",
    "            \"isIndexable\": False,\n",
    "            \"typeName\":\"string\",\n",
    "            \"valuesMaxCount\":1,\n",
    "            \"cardinality\":\"SINGLE\",\n",
    "            \"valuesMinCount\":0\n",
    "        }]\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_entity_dict = {\n",
    "  \"entity\" : {\n",
    "    \"guid\" : \"-2089428075574888\",\n",
    "    \"status\" : \"ACTIVE\",\n",
    "    \"createdBy\" : \"pdefusco\",\n",
    "    \"updatedBy\" : \"pdefusco\",\n",
    "    \"createTime\" : \"12342\",\n",
    "    \"updateTime\" : \"12342\",\n",
    "    \"version\" : \"12342\",\n",
    "    \"relationshipAttributes\" : {},\n",
    "    \"classifications\" : [],\n",
    "    \"typeName\" : \"Write_to_Impala\",\n",
    "    \"attributes\" : {\n",
    "      \"startTime\" : \"123\",\n",
    "      \"qualifiedName\": \"Write_to_Impala\",\n",
    "      \"name\":\"Write_to_Impala\",\n",
    "      \"description\":\"Record Linkage Algorithm\",\n",
    "      \"owner\": \"pdefusco\",\n",
    "        #, \n",
    "      \"inputs\":[{\"guid\": \"740ab8a4-c752-4ee7-b6be-6f4082ec9fee\", \"typeName\":\"hive_table\"}], \n",
    "      \"outputs\":[{\"guid\":\"\", \"type_name\":\"hive_table\"}]\n",
    "    }, \n",
    "  },\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/ER_atlas_lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can optionally remove the EM Algorithm instance from Atlas via the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = client.entity_guid(\"44848fe5-6950-4a73-a89c-9775b736b4c9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdefusco'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.entity['attributes'][\"owner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have completed our introduction to Splink and the Atlas Client. \n",
    "## Next we will simulate a real world Application with CML Jobs and COD (Cloudera Operational Database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
