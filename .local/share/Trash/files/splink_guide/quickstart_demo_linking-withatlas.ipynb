{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splink data linking demo (link only)\n",
    "\n",
    "In this demo we link two small datasets.  \n",
    "\n",
    "We assume we have a list of people in one table who we want to find in a larger table.  It is assumed that due to transcription or other errors, there will often not be an exact match.\n",
    "\n",
    "The larger table contains duplicates, but in this notebook we use the `link_only` setting, so `splink` makes no attempt to deduplicate these records.    Note it is possible to simultaneously link and dedupe using the `link_and_dedupe` setting.\n",
    "\n",
    "**Important** Where deduplication is not required, `link_only` can provide an important performance boost by dramatically reducing the number of records which need to be compared.\n",
    "\n",
    "For example, if you wanted to link 10 records to 1,000, then the maximum number of comparisons that need to be made (i.e. with no blocking rules) is 10,000.  If you need to dedupe as well, that number would be n(n-1)/2 = 509,545.\n",
    "\n",
    "I print the output at each stage using `spark_dataframe.show()`.  This is for instructional purposes only - it degrades performance and shouldn't be used in a production setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Imports and setup\n",
    "\n",
    "The following is just boilerplate code that sets up the Spark session and sets some other non-essential configuration options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct Cloud Storage URL is: s3a://demo-aws-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import datetime\n",
    "\n",
    "#Extracting the correct URL from hive-site.xml\n",
    "tree = ET.parse('/etc/hadoop/conf/hive-site.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "for prop in root.findall('property'):\n",
    "    if prop.find('name').text == \"hive.metastore.warehouse.dir\":\n",
    "        storage = prop.find('value').text.split(\"/\")[0] + \"//\" + prop.find('value').text.split(\"/\")[2]\n",
    "\n",
    "print(\"The correct Cloud Storage URL is: {}\".format(storage))\n",
    "\n",
    "os.environ['STORAGE'] = storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conf=SparkConf()\n",
    "\n",
    "# Load in a jar that provides extended string comparison functions such as Jaro Winkler.\n",
    "# Splink\n",
    "#     conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#     conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar,jars/graphframes-0.6.0-spark2.3-s_2.11.jar')\n",
    "#conf.set('spark.driver.extraClassPath', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars', 'jars/scala-udf-similarity-0.0.6.jar')\n",
    "#conf.set('spark.jars.packages', 'graphframes:graphframes:0.6.0-spark2.3-s_2.11')\n",
    "\n",
    "#sc = SparkContext.getOrCreate(conf=conf)\n",
    "#sc.setCheckpointDir(\"temp_graphframes/\")\n",
    "\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Entity Resolution with Lineage\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.s3guard.ddb.region\",\"us-east-1\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\", os.environ['STORAGE'])\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .config(\"spark.jars\", \"jars/scala-udf-similarity-0.0.6.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register UDFs\n",
    "from pyspark.sql import types\n",
    "spark.udf.registerJavaFunction('jaro_winkler_sim', 'uk.gov.moj.dash.linkage.JaroWinklerSimilarity', types.DoubleType())\n",
    "spark.udf.registerJavaFunction('Dmetaphone', 'uk.gov.moj.dash.linkage.DoubleMetaphone', types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://100.100.29.251:20049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5.7.2.2.0-244</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>k8s://https://172.20.0.1:443</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Entity Resolution with Lineage</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f17841d3518>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "pd.options.display.max_columns = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "logging.basicConfig()  # Means logs will print in Jupyter Lab\n",
    "\n",
    "# Set to DEBUG if you want splink to log the SQL statements it's executing under the hood\n",
    "logging.getLogger(\"splink\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read in the data\n",
    "\n",
    "The `l` and `r` stand for 'left' and 'right.  It doesn't matter which of the two datasets you choose as the left, performance and results will be the same.\n",
    "\n",
    "‚ö†Ô∏è Note that `splink` makes the following assumptions about your data:\n",
    "\n",
    "-  There is a field containing a unique record identifier in each dataset\n",
    "-  The two datasets being linked have common column names - e.g. date of birth is represented in both datasets in a field of the same name.   In many cases, this means that the user needs to rename columns prior to using `splink`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+----------+------------+--------------------+-----+\n",
      "|unique_id|first_name|surname|       dob|        city|               email|group|\n",
      "+---------+----------+-------+----------+------------+--------------------+-----+\n",
      "|        0|    Julia |   null|2015-10-29|      London| hannah88@powers.com|    0|\n",
      "|        4|      oNah| Watson|2008-03-23|      Bolton|matthew78@ballard...|    1|\n",
      "|       13|    Molly |   Bell|2002-01-05|Peterborough|                null|    2|\n",
      "|       15| Alexander|Amelia |1983-05-19|     Glasgow|ic-mpbell@alleale...|    3|\n",
      "|       20|    Ol vri|ynnollC|1972-03-08|    Plymouth|derekwilliams@nor...|    4|\n",
      "+---------+----------+-------+----------+------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+----------+-------+----------+------+--------------------+-----+\n",
      "|unique_id|first_name|surname|       dob|  city|               email|group|\n",
      "+---------+----------+-------+----------+------+--------------------+-----+\n",
      "|        1|    Julia | Taylor|2015-07-31|London| hannah88@powers.com|    0|\n",
      "|        2|    Julia | Taylor|2016-01-27|London| hannah88@powers.com|    0|\n",
      "|        3|    Julia | Taylor|2015-10-29|  null|  hannah88opowersc@m|    0|\n",
      "|        5|     Noah | Watson|2008-03-23|Bolton|matthew78@ballard...|    1|\n",
      "|        6|    Watson|  Noah |2008-03-23|  null|matthew78@ballard...|    1|\n",
      "+---------+----------+-------+----------+------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_l = spark.read.parquet(\"data/fake_df_l.parquet\")\n",
    "df_r = spark.read.parquet(\"data/fake_df_r.parquet\")\n",
    "df_l.show(5)\n",
    "df_r.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3:  Configure splink using the `settings` object\n",
    "\n",
    "Most of `splink` configuration options are stored in a settings dictionary.  This dictionary allows significant customisation, and can therefore get quite complex.  \n",
    "\n",
    "üí• We provide an tool for helping to author valid settings dictionaries, which includes tooltips and autocomplete, which you can find [here](http://robinlinacre.com/splink_settings_editor/).\n",
    "\n",
    "Customisation overrides default values built into splink.  For the purposes of this demo, we will specify a simple settings dictionary, which means we will be relying on these sensible defaults.\n",
    "\n",
    "To help with authoring and validation of the settings dictionary, we have written a [json schema](https://json-schema.org/), which can be found [here](https://github.com/moj-analytical-services/splink/blob/master/splink/files/settings_jsonschema.json).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"link_type\": \"link_only\", \n",
    "    \"max_iterations\": 5,\n",
    "    \"blocking_rules\": [\n",
    "        'l.first_name = r.first_name',\n",
    "        'l.surname = r.surname',\n",
    "        'l.dob = r.dob'\n",
    "    ],\n",
    "    \"comparison_columns\": [\n",
    "       {\n",
    "        \"custom_name\": \"name_inversion\",\n",
    "        \"custom_columns_used\": [\"first_name\", \"surname\", \"dob\"],\n",
    "        \"case_expression\": \"CASE WHEN first_name_l = first_name_r AND surname_l = surname_r THEN 2 WHEN first_name_l = surname_r AND surname_l = first_name_r THEN 1 ELSE 0 END\",\n",
    "        \"num_levels\": 3\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"dob\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"city\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"email\"\n",
    "        }\n",
    "    ],\n",
    "    \"additional_columns_to_retain\": [\"group\"]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In words, this setting dictionary says:\n",
    "\n",
    "- We are performing a data linking task (the other options are `dedupe_only`, or `link_and_dedupe`)\n",
    "- Rather than generate all possible comparisons (the cartesian product of the input datasets), we are going restrict record comparisons to those generated by at least one of the rules in the specified array\n",
    "- When comparing records, we will use information from the `first_name`, `surname`, `dob`, `city` and `email` columns to compute a match score.\n",
    "- For `first_name` and `surname`, string comparisons will have three levels:\n",
    "    - Level 2: Strings are (almost) exactly the same\n",
    "    - Level 1: Strings are similar \n",
    "    - Level 0: No match\n",
    "- We will make adjustments for term frequencies on the `first_name` and `surname` columns\n",
    "- We will retain the `group` column in the results even though this is not used as part of comparisons.  This is a labelled dataset and `group` contains the true match - i.e. where group matches, the records pertain to the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Save the Two Datasets as Spark Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_l.write.format('parquet').mode(\"overwrite\").saveAsTable('ER_table_left')\n",
    "df_r.write.format('parquet').mode(\"overwrite\").saveAsTable('ER_table_right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.listTables(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Estimate match scores using the Expectation Maximisation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:splink.iterate:Iteration 0 complete\n",
      "INFO:splink.params:The maximum change in parameters was 0.4944548845291138 for key œÄ_gamma_name_inversion_prob_dist_match_level_0_probability\n",
      "INFO:splink.iterate:Iteration 1 complete\n",
      "INFO:splink.params:The maximum change in parameters was 0.054529160261154175 for key œÄ_gamma_city_prob_dist_match_level_0_probability\n",
      "INFO:splink.iterate:Iteration 2 complete\n",
      "INFO:splink.params:The maximum change in parameters was 0.026873305439949036 for key œÄ_gamma_city_prob_dist_match_level_0_probability\n",
      "INFO:splink.iterate:Iteration 3 complete\n",
      "INFO:splink.params:The maximum change in parameters was 0.016269318759441376 for key œÄ_gamma_email_prob_dist_match_level_0_probability\n",
      "INFO:splink.iterate:Iteration 4 complete\n",
      "INFO:splink.params:The maximum change in parameters was 0.013959884643554688 for key œÄ_gamma_email_prob_dist_match_level_1_probability\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[match_probability: double, unique_id_l: bigint, unique_id_r: bigint, first_name_l: string, first_name_r: string, surname_l: string, surname_r: string, dob_l: string, dob_r: string, gamma_name_inversion: int, prob_gamma_name_inversion_non_match: double, prob_gamma_name_inversion_match: double, gamma_dob: int, prob_gamma_dob_non_match: double, prob_gamma_dob_match: double, city_l: string, city_r: string, gamma_city: int, prob_gamma_city_non_match: double, prob_gamma_city_match: double, email_l: string, email_r: string, gamma_email: int, prob_gamma_email_non_match: double, prob_gamma_email_match: double, group_l: bigint, group_r: bigint, match_key: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from splink import Splink\n",
    "\n",
    "linker = Splink(settings, spark, df_l=df_l, df_r=df_r)\n",
    "df_e = linker.get_scored_comparisons()\n",
    "\n",
    "# Later, we will make term frequency adjustments.  \n",
    "# Persist caches these results in memory, preventing them having to be recomputed when we make these adjustments.\n",
    "df_e.persist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.write.format('parquet').mode(\"overwrite\").saveAsTable('ER_target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inspect results \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_probability</th>\n",
       "      <th>unique_id_l</th>\n",
       "      <th>unique_id_r</th>\n",
       "      <th>first_name_l</th>\n",
       "      <th>first_name_r</th>\n",
       "      <th>surname_l</th>\n",
       "      <th>surname_r</th>\n",
       "      <th>dob_l</th>\n",
       "      <th>dob_r</th>\n",
       "      <th>gamma_name_inversion</th>\n",
       "      <th>prob_gamma_name_inversion_non_match</th>\n",
       "      <th>prob_gamma_name_inversion_match</th>\n",
       "      <th>gamma_dob</th>\n",
       "      <th>prob_gamma_dob_non_match</th>\n",
       "      <th>prob_gamma_dob_match</th>\n",
       "      <th>city_l</th>\n",
       "      <th>city_r</th>\n",
       "      <th>gamma_city</th>\n",
       "      <th>prob_gamma_city_non_match</th>\n",
       "      <th>prob_gamma_city_match</th>\n",
       "      <th>email_l</th>\n",
       "      <th>email_r</th>\n",
       "      <th>gamma_email</th>\n",
       "      <th>prob_gamma_email_non_match</th>\n",
       "      <th>prob_gamma_email_match</th>\n",
       "      <th>group_l</th>\n",
       "      <th>group_r</th>\n",
       "      <th>match_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>632</td>\n",
       "      <td>100</td>\n",
       "      <td>Noah</td>\n",
       "      <td>Noah</td>\n",
       "      <td>Gibson</td>\n",
       "      <td>None</td>\n",
       "      <td>1987-05-18</td>\n",
       "      <td>1983-07-20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992115</td>\n",
       "      <td>0.655863</td>\n",
       "      <td>0</td>\n",
       "      <td>0.951687</td>\n",
       "      <td>0.143546</td>\n",
       "      <td>London</td>\n",
       "      <td>Birmingha</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821861</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>avazquez@banks.com</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>107</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>352</td>\n",
       "      <td>578</td>\n",
       "      <td>Wallace</td>\n",
       "      <td>Williams</td>\n",
       "      <td>George</td>\n",
       "      <td>George</td>\n",
       "      <td>2018-10-31</td>\n",
       "      <td>1981-08-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992115</td>\n",
       "      <td>0.655863</td>\n",
       "      <td>0</td>\n",
       "      <td>0.951687</td>\n",
       "      <td>0.143546</td>\n",
       "      <td>StkeTon--rent</td>\n",
       "      <td>London</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821861</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>jonathan74@glover.com</td>\n",
       "      <td>derek58@gibbs.biz</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985161</td>\n",
       "      <td>0.133354</td>\n",
       "      <td>61</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>960</td>\n",
       "      <td>335</td>\n",
       "      <td>Gabriel</td>\n",
       "      <td>Gabriel</td>\n",
       "      <td>Bartlett</td>\n",
       "      <td>Potter</td>\n",
       "      <td>1973-12-09</td>\n",
       "      <td>1977-12-21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992115</td>\n",
       "      <td>0.655863</td>\n",
       "      <td>0</td>\n",
       "      <td>0.951687</td>\n",
       "      <td>0.143546</td>\n",
       "      <td>Wolverhampton</td>\n",
       "      <td>Leed</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821861</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>ogomez@robinson-mckinney.com</td>\n",
       "      <td>None</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>173</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>868</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>721</td>\n",
       "      <td>364</td>\n",
       "      <td>Sienna</td>\n",
       "      <td>Nancy</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>2005-06-04</td>\n",
       "      <td>1989-07-25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992115</td>\n",
       "      <td>0.655863</td>\n",
       "      <td>0</td>\n",
       "      <td>0.951687</td>\n",
       "      <td>0.143546</td>\n",
       "      <td>Liverpool</td>\n",
       "      <td>London</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821861</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>javierfrederick@kelly.org</td>\n",
       "      <td>wagnershane@landry.com</td>\n",
       "      <td>0</td>\n",
       "      <td>0.985161</td>\n",
       "      <td>0.133354</td>\n",
       "      <td>124</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>913</td>\n",
       "      <td>0.991286</td>\n",
       "      <td>419</td>\n",
       "      <td>423</td>\n",
       "      <td>Emily</td>\n",
       "      <td>None</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Brown</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>2005-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.992115</td>\n",
       "      <td>0.655863</td>\n",
       "      <td>1</td>\n",
       "      <td>0.048313</td>\n",
       "      <td>0.856454</td>\n",
       "      <td>Lndon</td>\n",
       "      <td>London</td>\n",
       "      <td>0</td>\n",
       "      <td>0.821861</td>\n",
       "      <td>0.218104</td>\n",
       "      <td>sarahbrown@mckinney.com</td>\n",
       "      <td>sarahbron@mckinney.com</td>\n",
       "      <td>1</td>\n",
       "      <td>0.014839</td>\n",
       "      <td>0.866646</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      match_probability  unique_id_l  unique_id_r first_name_l first_name_r  \\\n",
       "54             0.016302          632          100        Noah         Noah    \n",
       "1070           0.002238          352          578      Wallace     Williams   \n",
       "213            0.016302          960          335     Gabriel      Gabriel    \n",
       "868            0.002238          721          364      Sienna        Nancy    \n",
       "913            0.991286          419          423       Emily          None   \n",
       "\n",
       "     surname_l surname_r       dob_l       dob_r  gamma_name_inversion  \\\n",
       "54      Gibson      None  1987-05-18  1983-07-20                     0   \n",
       "1070   George    George   2018-10-31  1981-08-06                     0   \n",
       "213   Bartlett    Potter  1973-12-09  1977-12-21                     0   \n",
       "868     Taylor    Taylor  2005-06-04  1989-07-25                     0   \n",
       "913      Brown     Brown  2005-07-15  2005-07-15                     0   \n",
       "\n",
       "      prob_gamma_name_inversion_non_match  prob_gamma_name_inversion_match  \\\n",
       "54                               0.992115                         0.655863   \n",
       "1070                             0.992115                         0.655863   \n",
       "213                              0.992115                         0.655863   \n",
       "868                              0.992115                         0.655863   \n",
       "913                              0.992115                         0.655863   \n",
       "\n",
       "      gamma_dob  prob_gamma_dob_non_match  prob_gamma_dob_match  \\\n",
       "54            0                  0.951687              0.143546   \n",
       "1070          0                  0.951687              0.143546   \n",
       "213           0                  0.951687              0.143546   \n",
       "868           0                  0.951687              0.143546   \n",
       "913           1                  0.048313              0.856454   \n",
       "\n",
       "             city_l     city_r  gamma_city  prob_gamma_city_non_match  \\\n",
       "54           London  Birmingha           0                   0.821861   \n",
       "1070  StkeTon--rent     London           0                   0.821861   \n",
       "213   Wolverhampton       Leed           0                   0.821861   \n",
       "868       Liverpool     London           0                   0.821861   \n",
       "913           Lndon     London           0                   0.821861   \n",
       "\n",
       "      prob_gamma_city_match                       email_l  \\\n",
       "54                 0.218104            avazquez@banks.com   \n",
       "1070               0.218104         jonathan74@glover.com   \n",
       "213                0.218104  ogomez@robinson-mckinney.com   \n",
       "868                0.218104     javierfrederick@kelly.org   \n",
       "913                0.218104       sarahbrown@mckinney.com   \n",
       "\n",
       "                     email_r  gamma_email  prob_gamma_email_non_match  \\\n",
       "54                      None           -1                    1.000000   \n",
       "1070       derek58@gibbs.biz            0                    0.985161   \n",
       "213                     None           -1                    1.000000   \n",
       "868   wagnershane@landry.com            0                    0.985161   \n",
       "913   sarahbron@mckinney.com            1                    0.014839   \n",
       "\n",
       "      prob_gamma_email_match  group_l  group_r match_key  \n",
       "54                  1.000000      107       20         0  \n",
       "1070                0.133354       61       96         1  \n",
       "213                 1.000000      173       57         0  \n",
       "868                 0.133354      124       62         1  \n",
       "913                 0.866646       71       71         1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect main dataframe that contains the match scores\n",
    "df_e.toPandas().sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `params` property of the `linker` is an object that contains a lot of diagnostic information about how the match probability was computed.  The following cells demonstrate some of its functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative representation of the parameters displays them in terms of the effect different values in the comparison vectors have on the match probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.bayes_factor_chart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If charts aren't displaying correctly in your notebook, you can write them to a file (by default splink_charts.html)\n",
    "params.all_charts_write_html_file(\"splink_charts.html\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also generate a report which explains how the match probability was computed for an individual comparison row.  \n",
    "\n",
    "Note that you need to convert the row to a dictionary for this to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial probability of match (prior) = Œª = 0.3851017951965332\n",
      "\n",
      "Comparison of name_inversion.  Values are:\n",
      "name_inversion_l: Patel, Mohammed , 1989-03-19\n",
      "name_inversion_r: Mohammed , Patel, 1989-03-19\n",
      "Comparison has 3 levels\n",
      "ùõæ for this comparison = gamma_name_inversion = 1\n",
      "Amongst matches, m = P(ùõæ|match) = 0.12372622638940811\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 2.1532594018935924e-06\n",
      "Bayes factor = m/u = 57459.97267240647\n",
      "New probability of match (updated belief): 0.9999722124537389\n",
      "\n",
      "Comparison of dob.  Values are:\n",
      "dob_l: 1989-03-19\n",
      "dob_r: 1989-03-19\n",
      "Comparison has 2 levels\n",
      "ùõæ for this comparison = gamma_dob = 1\n",
      "Amongst matches, m = P(ùõæ|match) = 0.856454074382782\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 0.048313207924366\n",
      "Bayes factor = m/u = 17.727120826328797\n",
      "New probability of match (updated belief): 0.9999984324428572\n",
      "\n",
      "Comparison of city.  Values are:\n",
      "city_l: Sheffield\n",
      "city_r: Sheffield\n",
      "Comparison has 2 levels\n",
      "ùõæ for this comparison = gamma_city = 1\n",
      "Amongst matches, m = P(ùõæ|match) = 0.78189617395401\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 0.17813900113105774\n",
      "Bayes factor = m/u = 4.38924754820403\n",
      "New probability of match (updated belief): 0.9999996428638341\n",
      "\n",
      "Comparison of email.  Values are:\n",
      "email_l: None\n",
      "email_r: heather31@cabrera-reed.com\n",
      "Comparison has 2 levels\n",
      "ùõæ for this comparison = gamma_email = -1\n",
      "Amongst matches, m = P(ùõæ|match) = 1.0\n",
      "Amongst non matches, u = P(ùõæ|non-match) = 1.0\n",
      "Bayes factor = m/u = 1.0\n",
      "New probability of match (updated belief): 0.9999996428638341\n",
      "\n",
      "Final probability of match = 0.9999996428638341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from splink.intuition import intuition_report\n",
    "row_dict = df_e.toPandas().sample(1).to_dict(orient=\"records\")[0]\n",
    "print(intuition_report(row_dict, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from splink.diagnostics import splink_score_histogram\n",
    "from pyspark.sql.functions import expr \n",
    "splink_score_histogram(df_e.filter(expr('match_probability > 0.001')), spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Custom Atlas Type (Process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to instantiate the connection to Atlas in CDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atlasclient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endpoint, Username and Passoword are stored as CML project variables and passed dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from atlasclient.client import Atlas\n",
    "client = Atlas(os.environ[\"atlas_endpoint\"], port='', username=os.environ[\"atlas_username\"], password=os.environ[\"atlas_password\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the Client connection is working by querying a random Atlas entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "guid = \"c845eb62-d85d-4591-8abe-0c31449cdd95\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = client.entity_guid(guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:atlasclient.base:Missing attr EntityGuid: searchParameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'createTime': 1607119259591,\n",
       " 'description': None,\n",
       " 'displayName': None,\n",
       " 'metadata': {'default_project_engine_type': 'legacy_engine',\n",
       "  'project_description': 'Build an XGBoost model to predict churn using customer telco data.',\n",
       "  'project_visibility': 'private',\n",
       "  'service_name': 'mlgov/mlgovapiserver.mlx.cloudera.site@DEMO-AWS.YLCU-ATMI.CLOUDERA.SITE',\n",
       "  'session_username': 'alexbleakley'},\n",
       " 'modifiedTime': 1607119259591,\n",
       " 'name': 'Churn Modeling with XGBoost - alexbleakley',\n",
       " 'owner': 'alexbleakley',\n",
       " 'qualifiedName': 'crn:cdp:ml:us-west-1:8a1e15cd-04c2-48aa-8f35-b4a8c11997d3:workspace:ab34ab61-368e-46a2-923e-6c5830585734/affaa215-c794-436c-af69-dcccfffb3d02',\n",
       " 'replicatedFrom': None,\n",
       " 'replicatedTo': None,\n",
       " 'userDescription': None}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.entity['attributes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have successfully established the connection. Next we can create a custom Atlas type (process) reflecting the EM algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "typedef_dict = {\n",
    "    \"enumTypes\": [],\n",
    "    \"structTypes\": [],\n",
    "    \"classificationDefs\":[],\n",
    "    \"entityDefs\": [{\n",
    "        \"superTypes\": [\"Process\"],\n",
    "        \"name\": \"EM_algorithm_linkage\",\n",
    "        \"description\":\"custom_type_for_Entity_Resolution\",\n",
    "        \"attributeDefs\": [{\n",
    "            \"name\": \"startTime\",\n",
    "            \"isOptional\": True,\n",
    "            \"isUnique\": False,\n",
    "            \"isIndexable\": False,\n",
    "            \"typeName\":\"string\",\n",
    "            \"valuesMaxCount\":1,\n",
    "            \"cardinality\":\"SINGLE\",\n",
    "            \"valuesMinCount\":0\n",
    "        }]\n",
    "    }]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now register the new type with Atlas. For more on the Atlas type model, please visit this page: https://docs.cloudera.com/runtime/7.2.7/cdp-governance-overview/topics/atlas-metadata-model-overview.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Has already run once so will not run again\n",
    "#client.typedefs.create(data=typedef_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Instantiate the EM algorithm in Atlas along with lineage reflecting our Linkage Job above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: we need to pass the Atlas guid for the two datasets we compared above as they were registered in Atlas when they were stored as a Spark table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieving GUID's for the three tables via Atlas Client - search by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'typeName': 'hive_table', 'attrName': 'data', 'attrValue': 'provider', 'offset': '1', 'limit':'10'}\n",
    "#search_results = client.search_basic(**params)\n",
    "#for s in search_results:\n",
    "#    for e in s.entities:\n",
    "#        print(e.guid)\n",
    "#        print(e.attributes)\n",
    "#        print(e.attributes.values)\n",
    "#        print(e.typeName)\n",
    "#        print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'typeName': 'hive_table', 'attrName': 'name', 'attrValue': 'cc_data', 'offset': '1', 'limit':'10'}\n",
    "#search_results = client.search_attribute(**params)\n",
    "#for s in search_results:\n",
    "#    for e in s.entities:\n",
    "#        print(e.guid)\n",
    "#        print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for s in search_results:\n",
    "#    print(s.entities.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = {'typeName': 'hive_table', 'attrName': 'name', 'attrValue': 'cc_data', 'offset': '1', 'limit': '100'}\n",
    "#search_results = client.search_basic.create(data=data)\n",
    "#for e in search_results.entities:\n",
    "#    print(e.guid)\n",
    "#    print(e.attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_entity_dict = {\n",
    "  \"entity\" : {\n",
    "    \"guid\" : \"-2089428075574333\",\n",
    "    \"status\" : \"ACTIVE\",\n",
    "    \"createdBy\" : \"pdefusco\",\n",
    "    \"updatedBy\" : \"pdefusco\",\n",
    "    \"createTime\" : \"12342\",\n",
    "    \"updateTime\" : \"12342\",\n",
    "    \"version\" : \"12342\",\n",
    "    \"relationshipAttributes\" : {},\n",
    "    \"classifications\" : [],\n",
    "    \"typeName\" : \"EM_algorithm_linkage\",\n",
    "    \"attributes\" : {\n",
    "      \"startTime\" : \"123\",\n",
    "      \"qualifiedName\": \"EM Record Linkage\",\n",
    "      \"name\":\"EM Record Linkage\",\n",
    "      \"description\":\"Record Linkage Algorithm\",\n",
    "      \"owner\": \"pdefusco\",\n",
    "        #, \n",
    "      \"inputs\":[{\"guid\": \"aa955089-5a11-46d9-9dbf-2f6b75f4d65b\", \"typeName\":\"hive_table\"},\n",
    "               {\"guid\": \"43d788ce-4af4-4253-af0b-465ea45c1b93\", \"typeName\":\"hive_table\"}], \n",
    "      \"outputs\":[{\"guid\":\"ac1bdcb3-73c8-4198-a8e6-0aa104c606bb\", \"type_name\":\"hive_table\"}]\n",
    "    }, \n",
    "  },\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guidAssignments': {'-2089428075574333': '44848fe5-6950-4a73-a89c-9775b736b4c9'}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.entity_post.create(data=process_entity_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Navigate to Atlas (SDX) and browse for the \"EM_algorithm_linkage\" entity. Expand the lineage tab and the source and target datasets will be shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/ER_atlas_lineage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can optionally remove the EM Algorithm instance from Atlas via the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = client.entity_guid(\"44848fe5-6950-4a73-a89c-9775b736b4c9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pdefusco'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity.entity['attributes'][\"owner\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have completed our introduction to Splink and the Atlas Client. \n",
    "## Next we will simulate a real world Application with CML Jobs and COD (Cloudera Operational Database)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
