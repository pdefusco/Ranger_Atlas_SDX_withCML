{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising configuration and best practices\n",
    "\n",
    "It is rare that Splink's default settings will provide the best data linking results.  Significant improvements can usually be made with careful attention to data cleaning, comparison functions, and other configuration options.  \n",
    "\n",
    "This notebook contains advice about best practice to help users get the more accurate data linkage results. It is based on experiences in optimising real-world jobs.\n",
    "\n",
    "It focusses on introducing concepts and building intuition instead of providing extensive code or exhaustive details of every config option.  Please see the other examples in this repository for fully-working examples of code, and [here](https://moj-analytical-services.github.io/splink_settings_editor/) for a full list of configuration options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "The [EM algorithm](https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm) that Splink uses to estimate model parameters is a unsupervised machine learning algorithm.  Specifically, it learns an optimal set of matching weights from the dataset of record comparisons.  \n",
    "\n",
    "This helps structure our thinking about how best to use Splink because it means we can dip into the standard set tools and ways of thinking that we use for any machine learning problem.\n",
    "\n",
    "Two things are particularly relevant for optimising of a Splink job:\n",
    "\n",
    "- Feature engineering:  How to transform our data to help Splink learn as much as possible.\n",
    "- How can we avoid confusing Splink with bad data or configuration?  (Overfitting, converging to local rather than global maxima, etc.)\n",
    "\n",
    "A second ingredient to help structure our thoughts is an better understanding of what Splink is trying to learn.\n",
    "\n",
    "In a nutshell, Splink looks to exploit differences in match probability between subsets of record comparisons.  The greater the differences, the more accurate the matching.\n",
    "\n",
    "This means that the task of the user is generally to try and make these groups as different as possible.\n",
    "\n",
    "Splink combines information on different groups using the assumption of conditional independence, which means it just multiplies all the probabilities together.   \n",
    "\n",
    "For instance, if we observe a match on first name AND surname, Splink separately computes:\n",
    "- The amount of evidence in favour of a match provided by the match on first name\n",
    "- The amount of evidence in favour of a match provided by the match on surname\n",
    "\n",
    "and multiples these probabilities together.\n",
    "\n",
    "This is often unrealistic because columns can be correlated, and so can result in double counting. A large part of optimising a Splink job is configuring the job to mitigate the impact of the failure of the conditional independence assumption.   For example, we’ll see later that including two different columns that are both measures of a person’s location can cause problems.\n",
    "\n",
    "\n",
    "\n",
    "These concepts go a long way to explaining most of the optimisations in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our example data\n",
    "\n",
    "In the remainder of this notebook we will build up an example settings dictionary, based on an example dataset like this which we want to dedupe (only the first two rows are shown):\n",
    "\n",
    "\n",
    "| first_name | surname | initials | gender | dob        | postcode | office           |\n",
    "|------------|---------|----------|--------|------------|----------|------------------|\n",
    "| robin      | linacre | rl       | M      | 2000-01-01 | TA12 9PD | Bristol          |\n",
    "| john       | smith   | js       | NULL   | 1955-02-03 | BA8      | Manchester       |\n",
    "\n",
    "\n",
    "### A note on default settings\n",
    "\n",
    "Many of the settings described in this notebook have 'sensible defaults' which Splink uses if the user does not provide an explicit value.  You can always find out what these sensible defaults are by using an inbuilt function which populates all empty settings with their default values.  [This](https://github.com/moj-analytical-services/splink_demos/blob/master/code_snippets/completing_settings.py) code snippet demonstrates how to do this.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Blocking rules\n",
    "\n",
    "The first task is to generate a dataset of pairwise record comparisons.   The main purpose of Splink is to estimate a match probability for each record comparison in this dataset.\n",
    "\n",
    "Here is an example of a record comparison:\n",
    "\n",
    "| first_name_l | first_name_r | surname_l | surname_r | initials_l |initials_r | gender_l | gender_r | dob_l      | dob_r      | postcode_l | postcode_r | office_l           | office_r         |\n",
    "|--------------|--------------|-----------|-----------|------------|-----------|----------|----------|------------|------------|------------|------------|--------------------|------------------|\n",
    "| robin        | john         | linacre   | smith     | rl         |js         | M        | NULL     | 2000-01-01 | 1955-02-03 | TA12 9PD   | BA8        | Bristol            | Manchester       |\n",
    "\n",
    "\n",
    "For small datasets of perhaps around 1,000 records, it is computationally tractable to compare every record with every other.  The number of records generated will be $\\frac{n\\left(n-1\\right)}2$ , or 499,500 comparisons for 1,000 input records.  In this case, blocking rules can be ommitted entirely, and this is  probably the simplest and best way to proceed.\n",
    "\n",
    "For larger datasets, the problem quickly becomes computationally intractable and a more efficient approach is required.  We can attempt to restrict comparisons to only records which may plausibly be matches, using an approach called 'blocking' (see [here](https://toolkit.data.gov.au/Data_Linking_Information_Series_Sheet_4:_Probabilistic_linking.html)).\n",
    "\n",
    "For instance, if the date of birth field is always complete and is always accurate, then only records where date of birth matches are plausible matches.\n",
    "\n",
    "This can be specified in the Splink settings dictionary like so:\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules\": ['l.dob = r.dob']\n",
    "}\n",
    "```\n",
    "\n",
    "This eliminates all comparisons where date of birth does not match.  \n",
    "\n",
    "In the real world, this comes with the risk that we might miss potential duplicates - either due to missingness in the date of birth field, or typos or other errors in the data.\n",
    "\n",
    "This leads to a problem - empirically, it is usually almost impossible to find a single blocking rule which (a) reduced the number of comparisons to a managable level and (b) does not make any mistakes (i.e. does not remove any true matches).\n",
    "\n",
    "Splink therefore allows the user to specify multiple blocking rules.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules\": [\n",
    "        'l.dob = r.dob',\n",
    "        'l.office = r.office and l.initials = r.initials'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "The above settings objects generates:\n",
    "- all comparisons where date of birth matches\n",
    "- all comparisons where both office and initials match\n",
    "\n",
    "It then vertically concatenates these datasets, deduplicating them so that comparisons appear only once.\n",
    "\n",
    "**⚠️ POTENTIAL TRAP ⚠️ :**  A lot of care must be taken when using multiple blocking rules.  The explanation for this is nuanced and relies on concepts introduced later in this notebook.  It is therefore discussed further under the header 'a deeper dive into blocking rules', below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Decide on a list of comparison columns\n",
    "\n",
    "The user must decide which columns to include in the list of `comparison_columns`.\n",
    "\n",
    "Each comparison column represents a way of splitting the data into different groups, to try and find groups which contain different proportions of matches.  For example, by including `first_name` in the list of comparison columns, Splink will compare the subset of record comparisons where first name matches with the subset of record comparisons where first name does not match.  The aim is for Splink to work out how much evidence is contained the first name column which can help in assessing the match probability.\n",
    "\n",
    "It is usually best to include any columns containing information that may help us accept or reject a match.\n",
    "\n",
    "**⚠️ POTENTIAL TRAP ⚠️ :**  Do not include columns that repeat information in other columns because this will then be double counted. In this case, the person's initials should not be included as a separate comparison column.  \n",
    "\n",
    "Where columns are highly correlated, consider including only one, since the Fellegi Sunter model assumes independence.  Violation of this assumption can often result in some degree of double counting.  In this case, office is likely to be highly correlated with postcode, so we would advice against inclusion.\n",
    "\n",
    "\n",
    "\n",
    "At this stage, we have the following settings\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"surname\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"gender\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"dob\"\n",
    "        },\n",
    "        {\n",
    "            \"col_name\": \"postcode\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "#### The interaction of blocking rules and comparison columns\n",
    "\n",
    "**⚠️ POTENTIAL TRAP ⚠️ :**  It is generally inadvisable to include columns you've blocked on as comparison columns.  Often, the job will fail.\n",
    "\n",
    "To see why, consider a job that blocks on date of birth, and also includes date of birth in the list of comparison columns.  Splink will be looking for differences between record comparisons where date of birth matches vs record comparisons where date of birth does not match.  However, the blocking rule enforces the condition that date of birth must match, so Splink will not be able to compare these two groups of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Chose the number of levels for each comparison column\n",
    "\n",
    "Next, the user much choose the number of levels for each elements of the `comparison_columns` list. \n",
    "\n",
    "Consider specifically the `first_name` and `gender` comparison columns:\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    ...\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\",\n",
    "            \"num_levels\": 3\n",
    "        },\n",
    "        ...\n",
    "        {\n",
    "            \"col_name\": \"gender\",\n",
    "            \"num_levels\": 2\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "What does `num_levels` mean and why may we want three for the name columns, but two for gender?\n",
    "\n",
    "Remember we are seeking to exploit differenecs in the distribution of matches between different subsets of record comparisons.\n",
    "\n",
    "For a first_name field, it's reasonable to assume there may be different match rates among three groups:\n",
    "- The group of record comparisons where first name matches exactly.\n",
    "- The group of record comparisons where first names are similar but not exactly the same\n",
    "- The group of record comparisons where first names are not similar.\n",
    "\n",
    "`\"num_levels\": 3` creates these groups, enabling Splink to estimate different match weights for each group.\n",
    "\n",
    "Another way to understand why three levels is important is to consider the cost of using only two levels:\n",
    "- The group of record comparisons where first name matches exactly.\n",
    "- The group of record comparisons where first name does not match exactly.\n",
    "\n",
    "The later group contains both record comparisons where the name almost matches, and ones where names does match at all.  The algorithm has to 'take an average' - which will result in the estimated match probability for 'almost matches' being scored down too harshly, and the 'does not match at all' records not being scored down enough.\n",
    "\n",
    "For the gender column, which contains 'M', 'F', or null, only a two-level comparison is reasonable: it either matches or it doesn't.  \n",
    "\n",
    "### Tradeoffs\n",
    "\n",
    "Taken to its logical extreme, we could consider having a very large number of levels to account for subtle difference in distributions between groups of records with slighly different characterstics (e.g. one group for each possible value of edit distance).\n",
    "\n",
    "There are two downsides to increasing the number of levels:\n",
    "- Overfitting.  The more parameters you have, the more likely estimates are to be influenced by specific characteristics of your training data.  With many levels, some groups will be very small, which can lead to extreme parameter estimates.\n",
    "- Computational complexity.  More levels means longer compute times.\n",
    "\n",
    "Empirically, we have found that 3 or 4 levels is generally suitable for columns where string comparison functions are being used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Customising the comparison \n",
    "\n",
    "Until now, we have not been precise about how record comparisons are assigned to different levels.  What function is used to assign comparisons to different levels?  For instance, what do we mean by 'similar but not exactly the same'?\n",
    "\n",
    "In Splink, a SQL `CASE WHEN` expression is used for this purpose.\n",
    "\n",
    "An example of a three level statement may be:\n",
    "```python\n",
    "custom_case_expression = \"\"\"\n",
    "case\n",
    "when first_name_l is null or first_name_r is null then -1\n",
    "when jaro_winkler_sim(first_name_l, first_name_r) > 0.94 then 2\n",
    "when jaro_winkler_sim(first_name_l, first_name_r) > 0.88 then 1\n",
    "else 0 end\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "This can then be provided to the settings object as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    ...\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\",\n",
    "            \"num_levels\": 3\n",
    "            \"case_expression\": custom_case_expression\n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Splink provides default case expressions when the user does not provide the setting explicitly.  **However, we recommend that the user provides a custom `case_expression` for each comparison column**\n",
    "\n",
    "Splink provides a variety of functions that simply constructing these custom case expressions, which can be found in the `splink.case_statements` module [here](https://github.com/moj-analytical-services/splink/blob/master/splink/case_statements.py).\n",
    "\n",
    "For example, to generate the above statement, the user could have done this:\n",
    "\n",
    "```python\n",
    "from splink.case_statements import sql_gen_gammas_case_stmt_jaro_3\n",
    "print(sql_gen_gammas_case_stmt_jaro_3('first_name'))\n",
    "```\n",
    "\n",
    "**⚠️ POTENTIAL TRAP ⚠️ :**  Comparisons should always include explicit treatment of null values.  Otherwise null comparisons will often be assigned to level 0.  Level 0 would then be a mix of explicit non-matches, and nulls.\n",
    "\n",
    "Splink uses the 'special' level `-1` to indicate null comparisons.  Most often, Splink should consider the comparison as null if _either_ value is missing using a statement like that provided above:  `when first_name_l is null or first_name_r is null then -1`\n",
    "\n",
    "\n",
    "#### Writing an effective case expression\n",
    "\n",
    "Recall that we want to create subsets of record comparisons which have different proportions of matches vs. non matches - the more different the better.\n",
    "\n",
    "For example, consider the case of a three-level case expression for first name.\n",
    "\n",
    "We know that names can often be misspelt, or nicknames or diminutive forms may be used (e.g. Robin vs Robyn).  \n",
    "\n",
    "It therefore probably does not make sense to group comparisons of Robin vs Robyn with comparisons where there is no similarity (Robin vs John) because the former group is likely to have a reasonably high match rate, whereas the later group is likely to have a low match rate. \n",
    "\n",
    "As such, we want to craft a case statement that 'picks out' the relevant comparisons as precisely as possible.  \n",
    "\n",
    "For instance, we probably want to distinguish between 'Robin vs Robyn' and 'Freddy' vs 'Teddy'.\n",
    "\n",
    "By choosing the right string comparison function we can ensure that 'Robin vs Robyn' is assigned to level 1 (some similarity), and 'Freddy' vs 'Teddy' is assigned to level 0 (no similarity).  A combiantion of `Dmetaphone` and `jaro_winkler_sim` may be appropriate here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String comparison functions provided with Splink\n",
    "\n",
    "A variety of common string comparison functions are provided in a [jar](https://github.com/moj-analytical-services/splink/tree/master/jars) accompanying splink. Since these are implmeneted in Java/Scala, they should be much faster than using equivalent Python functions.\n",
    "\n",
    "A code snippet showing how to load them into Spark can be found [here](https://github.com/moj-analytical-services/splink_demos/blob/master/code_snippets/loading_jar.py). \n",
    "\n",
    "The functions provided are:\n",
    "- **[Jaro-Winkler similarity](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance#Jaro%E2%80%93Winkler_Similarity).**  A normalised edit distance running from 0 (no match) to 1 (exact match). Places greater weight on earlier characters. Generally a good option for names and dictionary words.\n",
    "- [**Jaccard similarity**](https://en.wikipedia.org/wiki/Jaccard_index).  A token-based similarity measure that considers the number of common tokens.  May be appropriate for some unique identifiers such as a driving licence number that may be entered with error.\n",
    "- [**Cosine distance**](https://en.wikipedia.org/wiki/Cosine_similarity).  A measure of similarity between different substrings.  Particularly appropriate for longer text strings that cannot be parsed out into separate fields.\n",
    "- [**Dmetaphone**](https://en.wikipedia.org/wiki/Metaphone).  A phonetic algorithm for indexing words by their English pronunciation.\n",
    "\n",
    "Splink defaults to using [Jaro-Winkler similarity](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance#Jaro%E2%80%93Winkler_Similarity) for string comparisons, which is generally a good choice for names or dictionary words.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom comparisons using more than one column\n",
    "\n",
    "We previously noted that the list of comparison columns should not contain multiple columns that measure the same thing, because this causes double counting in the match probability estimate.    \n",
    "\n",
    "For instance, in our example dataset the `office` and `postcode` fields are both essentially measures of the location of a person at different levels of granularity, so they should not be included as separate comparison columns.\n",
    "\n",
    "However, it's not uncommon to want to include information from both columns because a _combiantion_ of the columns provides a more accurate measure of location than either individual column.  In this example, perhaps the postcode column is often empty, but the office field is always complete.  We want to use postcode as a measure of location where the data exists, and fall back on office when the postcode is missing.\n",
    "\n",
    "We achieve this in Splink by passing a list of `custom_columns_used`, associated with a `custom_name` which is chosen by the user.  \n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "custom_case_expression = f\"\"\"\n",
    "case\n",
    "when (office_l is null or office_r is null) and (postcode_l is null or postcode_r is null) then -1\n",
    "when (postcode_l is not null and postcode_r is not null) and postcode_l = postcode_r then 2\n",
    "when office_l = office_r then 1\n",
    "else 0 end\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "settings = {...\n",
    "            \"comparison_columns\": [\n",
    "                {            \n",
    "                \"custom_name\": \"location_custom_comparison\"\n",
    "                \"case_expression\": custom_case_expression,\n",
    "                \"custom_columns_used\": [\"postcode\", \"office\"],\n",
    "                \"num_levels\": 3\n",
    "                }\n",
    "                ]\n",
    "           }\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Set starting parameters (m and u probabilities)\n",
    "\n",
    "It is good practice to set starting values for the model's parameters.  \n",
    "\n",
    "This has two advantages:\n",
    "\n",
    "- It means iteration is likely to be faster, resulting in less computation time\n",
    "- It helps with diagnostics:  if the model iterates a long way from starting estimates, it's an indication that something may be wrong.\n",
    "\n",
    "The starting values which should be set are:\n",
    "\n",
    "- λ, the overall proportion of the records comparisons created by the blocking rules which are matches\n",
    "- m and u probabilities for each comparison column.\n",
    "\n",
    "To understand what the m and u probabilities mean, consider the following settings:\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    ...\n",
    "    \"proportion_of_matches\": 0.1,\n",
    "    \"comparison_columns\": [\n",
    "        {\n",
    "            \"col_name\": \"first_name\",\n",
    "            \"num_levels\": 3\n",
    "            \"case_expression\": custom_case_expression,\n",
    "            \n",
    "            \"m_probabilities\": [0.25,  # 1st item in list is level 0\n",
    "                                0.20,  # 2nd item in list is level 1\n",
    "                                0.55], # 3rd item in list is level 2\n",
    "            \n",
    "            \n",
    "            \"u_probabilities\": [0.75,  # 1st item in list is level 0\n",
    "                                0.20,  # 2nd item in list is level 1\n",
    "                                0.05]  # 3rd item in list is level 2\n",
    "            \n",
    "        },\n",
    "        ...\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "This settings object says that we expect a record comparison drawn at random to have a 10% probability of being a match (`\"proportion_of_matches\": 0.1`).\n",
    "\n",
    "The m and u probabilities can be interpreted as follows:\n",
    "\n",
    "**Level 0**\n",
    "We expect 75% of non-matches and and 25% of matches to be categorised as level 0.  As such, if we observe level 0, this is fairly strong evidence against a match.\n",
    "\n",
    "**Level 1**\n",
    "We expect 20% of non-matches and and 20% of matches to be categorised as level 1.  Level 1  therefore provides no evidence of whether it's a match or not.\n",
    "\n",
    "\n",
    "**Level 2**\n",
    "We expect 5% of non-matches and and 55% of matches to be categorised as level 2.  A such, level 2 is strong evidence in favour of of a match.\n",
    "\n",
    "\n",
    "It can be difficult to manually choose these numbers, particularly where `num_levels` > 2, so we provide a tool to generate them more intuitively [here](https://observablehq.com/@robinl/m-and-u-probabilities).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency adjustments \n",
    "\n",
    "A common problem in data matching is unbalanecd term frequencies.  For example, a forename field with typically have an unbalanced distribution - with John appearing much more frequently than, say, Robin.\n",
    "\n",
    "The problem is that, without making term frequency adjustments, Splink will assume that all matches on name are equivalent.  In reality, it's likely that a match on John contains less information about whether the records match than a match on  Robin.\n",
    "\n",
    "Term frequency adjustments can be used in these cases of unbalanced distributions of data.  Note that term frequency adjustments are _not_ needed to take account of the cardinality of the data - Splink will take account of the cardinality of every field by default. (This is why estimated weights on very distinct fields like social security number will be greater than on a field like name.)  Term frequency adjustments make a further adjustment to account for the unevent distribution of terms.  Typically we will see match weights increase for uncommon terms and decrease for common terms.\n",
    "\n",
    "It is therefore good practice to set term frequency adjustment on all fields where there is substantial skew in term frequencies.\n",
    "\n",
    "It’s worth noting that this is essentially equivalent to having a different level for each unique name in the dataset.  However, the computational complexity of trying to estimate this during EM is too great, so they’re made as an ex-post adjustment after the main weights have been estimated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying problems\n",
    "\n",
    "Splink outputs a number of diagnostic charts and other information that can help the user identify problems. \n",
    "\n",
    "The easiest way to access these charts is by running the following code at the end of your job and loading the resulting html file:\n",
    "\n",
    "```python\n",
    "params.all_charts_write_html_file(filename=\"splink_charts.html\")\n",
    "```\n",
    "\n",
    "\n",
    "These help indicate where problems may be occuring or areas for improvement in the config and are a useful part of optimising a job.\n",
    "\n",
    "In addition, you should always perform analysis of the accuracy of the match by comparing Splink results against labelled or ground truth data.\n",
    "\n",
    "The advice in this section is heuristic and comes from experience in fixing problems with real-world jobs\n",
    "\n",
    "#### Heuristic 1: Convergence takes a long time\n",
    "\n",
    "If your job takes a large number of iterations to converge this is often an indication of an underlying problem with your setting object.  \n",
    "\n",
    "As a rule of thumb, problems are more likely when the number of iterations exceeds around 40.\n",
    "\n",
    "A typical 'well behaved' Splink job iterates in around 10-30 iterations an exhibits a initial pattern of rapidly changing parameters, changes getting smaller and smaller:\n",
    "\n",
    "<img src=\"images/well_behaved.png\" alt=\"Well behaved convergence\" style=\"width: 300px;\"/>\n",
    "\n",
    "The pattern of convergence can also be an indication that something is wrong - for instance, if parameter changes are relative constant or get smaller and then larger:\n",
    "\n",
    "<img src=\"images/convergence_issues.png\" alt=\"Convergence issues\" style=\"width: 300px;\"/>\n",
    "\n",
    "Such problems may indicate the likelihood function may be multi-modal or have a strange shape, which might be an indication of problems in your data or your configuration.  \n",
    "\n",
    "In our experience a common cause is the use of multiple blocking rules.\n",
    "\n",
    "\n",
    "\n",
    "#### Heuristic 2: m and u estimates are unexpected/unintuitive\n",
    "\n",
    "Unexpected parameter estimates are often an indicator of a problem.  For example, you may find that a non-match on a forename field is estimated to _increase_ the match probability.\n",
    "\n",
    "Unusual patterns in parameters can be caused by certain combinations of blocking rules and are not always indicative of a problem.  However, if you can't explain why they are exhibiting strange behaviour, this is usually an inidcation something is amiss.\n",
    "\n",
    "It is often useful to manually inspect some intuition reports, which can help get a sense of the overall results that are being produced by the parameter estimates.  The [data deduplication quick start](quickstart_demo_deduplication.ipynb) notebook contains an example of how to do this.\n",
    "\n",
    "\n",
    "#### Heuristic 3: Parameter estimates that implausibly converge to +inf or -inf\n",
    "\n",
    "In some jobs, the paremter estimates for some fields will converge to +inf or -inf.  \n",
    "\n",
    "Where this happens, this effectively means that a single field will drown out all others.  For instance, a +inf parameter estimate on first name will mean that if first name matches, the match probability will be estimated as 1.00 irrespective of the values of other fields.\n",
    "\n",
    "This is typically indicative of overfitting, unless there's a strong reason to believe this is really true.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A deeper dive into multiple blocking rules\n",
    "\n",
    "Good blocking rules eliminate large numbers of non-matching record comparisons without eliminating record comparisons which do match.  That is, we are looking for a decision rule with 100% recall, and with good enough sensitivity to reduce record comparisons to computationally tractable number.\n",
    "\n",
    "Unfortunately, it is rarely possible to find blocking rules which meet both of these criteria.  For example, blocking on date of birth will greatly reduce the number of record comparisons, but if typos are present in the date of birth field, the rule may eliminate a small number of true matches.\n",
    "\n",
    "This problem can be remedied by considering multiple blocking rules.  For instance, we may block on date of birth OR postcode.  Now the only true matches we miss are matches where there is an error in both the postcode and the date of birth.\n",
    "\n",
    "There are two broad approaches possible here:\n",
    "1. Running separate data matching exercise for each blocking rule.\n",
    "2. Generating a list of record comparisons which match any of the blocking rules, and running a single Splink job to estimate matching weights across all these record comparisons.\n",
    "\n",
    "Option 1 is theoretically more sound and is likely to result in more accurate matching results.  However, it results in significantly more complexity, because different settings are needed for each job, diagnostics need to be run on each job separately to check parameter estimates are sensible,  and the results from multiple jobs then need to be combined.\n",
    "Option 2 is much simpler and can often yield results which are 'good enough'.  However, in some situations it can result in completely nonsensical results.\n",
    "\n",
    "To understand why option 2 can be a bad idea, consider running a single Splink job whether we block on:\n",
    "\n",
    "Rule 1: Social security number \n",
    "Rule 2: Date of birth.  \n",
    "\n",
    "Assume that both fields are a result of manual data entry and are subject to transciption errors.\n",
    "\n",
    "The problem is that _same_ parameter estimates have to apply to all record comparisons, irrespective of the blocking rule that generated them.  \n",
    "\n",
    "This means we are likely to underestimate match probability for records \n",
    "\n",
    "To understand this, consider the subset of record comparisons in which date of birth does not match.  These record comparison must arise from blocking rule 1, which in turn means that social security number matches for all records in this subset.  As such, they are highly likely to be matches.  A non-match on date of birth is likely to be evidence _in favour_ of a match.  This implies that a match on date of birth is evidence _against_ a match.  \n",
    "\n",
    "This then leads to a result which is clearly incorrect. Compare the following two records comparisons:\n",
    "Comparison 1:  Social security number and date of birth match\n",
    "Comparison 2:  Social security number match, date of birth does not match\n",
    "\n",
    "Intuitively we expect comparison 1 to get a higher match probability, but in fact, comparison 2 is scored higher.\n",
    "\n",
    "This proves there are cases in which multiple blocking rules is a bad idea.  However, it does not prove it is always a bad idea - and we have found that, empirically, there are cases where multiple blocking rules are effective.\n",
    "\n",
    "\n",
    "### Empirical advice: When to use multiple blocking rules\n",
    "\n",
    "\n",
    "It's difficult to give advice on multiple blocking which applies to all situations.  The following rules of thumb may be useful:\n",
    "\n",
    "- Multiple blocking rules can lead to bad results when different blocking rules imply big differeces in conditional probabilities.  For example, a match on 'surname' may be much more informative when blocking on date of birth than when blocking on post code.\n",
    "- Multiple blocking rules seem to work better the more and varied they are\n",
    "- Multiple blocking rules seem to work better when the proportion of matches and non matches generated by each blocking rule is similar\n",
    "- The problems associated with multiple blocking rules seem more acute the fewer the number of comparison columns you have.  \n",
    "\n",
    "Because of the problems associated with multiple blocking rules, very careful attention should be taken to convergence and parameter estimates. \n",
    "\n",
    "One particuar situation in which have found the use of multiple blocking rules is effective is when strugging to find a single blocking rule that is appropriately strict.\n",
    "\n",
    "For instance, suppose blocking on postcode alone, or date of birth alone, result in too many record comparisons.\n",
    "\n",
    "In this case we may consider a series of blocking rules such as:\n",
    "\n",
    "```python\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules\": [\n",
    "        'l.dob = r.dob and l.forename_initial = r.forename_initial',\n",
    "        'l.dob = r.dob and l.surname_initial = r.surname_initial',\n",
    "        'l.dob = r.dob and l.outer_postcode = r.outer_postcode'\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "Which restricts comparisons down to a more managable number, whilst being very unlikely to reject true matches amongst the dte of birth block.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
